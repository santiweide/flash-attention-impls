==PROF== Connected to process 2459408 (/home/fng685/flash-attention-impls/code/zhigao yan/cudafa1)
==PROF== Profiling "flash_attention_forward" - 0: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 1: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 2: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 3: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 4: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 5: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 6: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 7: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 8: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 9: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 10: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 11: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 12: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 13: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 14: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 15: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 16: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 17: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 18: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 19: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 20: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 21: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 22: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 23: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 24: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 25: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 26: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 27: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 28: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 29: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 30: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 31: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 32: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 33: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 34: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 35: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 36: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 37: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 38: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 39: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 40: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 41: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 42: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 43: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 44: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 45: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 46: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 47: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 48: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 49: 0%....50%....100% - 10 passes
==PROF== Profiling "flash_attention_forward" - 50: 0%....50%....100% - 10 passes
Avg latency: 992.24 ms
throughput: 0.00 GB/s
compute: 0.541 GFLOPs/s
==PROF== Disconnected from process 2459408
[2459408] cudafa1@127.0.0.1
  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,858,280
    Memory Throughput                 %        12.39
    DRAM Throughput                   %         0.10
    Duration                         ms         6.35
    L1/TEX Cache Throughput           %        12.92
    L2 Cache Throughput               %         2.94
    SM Active Cycles              cycle 4,641,859.86
    Compute (SM) Throughput           %        10.27
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.61%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      7,926.20
    Total DRAM Elapsed Cycles        cycle   308,643,840
    Average L1 Active Cycles         cycle  4,641,859.86
    Total L1 Elapsed Cycles          cycle   522,704,666
    Average L2 Active Cycles         cycle  2,442,245.41
    Total L2 Elapsed Cycles          cycle   365,799,600
    Average SM Active Cycles         cycle  4,641,859.86
    Total SM Elapsed Cycles          cycle   522,704,666
    Average SMSP Active Cycles       cycle  2,752,683.63
    Total SMSP Elapsed Cycles        cycle 2,090,818,664
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.51%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.09% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.566%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 16.04% above the average, while the minimum instance value is 16.65% below  
          the average.                                                                                                  

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,852,283
    Memory Throughput                 %        12.37
    DRAM Throughput                   %         0.04
    Duration                         ms         6.34
    L1/TEX Cache Throughput           %        12.90
    L2 Cache Throughput               %         2.90
    SM Active Cycles              cycle 4,647,411.65
    Compute (SM) Throughput           %        10.25
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.70
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.71%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,049.40
    Total DRAM Elapsed Cycles        cycle   308,262,400
    Average L1 Active Cycles         cycle  4,647,411.65
    Total L1 Elapsed Cycles          cycle   523,486,904
    Average L2 Active Cycles         cycle  2,456,086.73
    Total L2 Elapsed Cycles          cycle   365,347,840
    Average SM Active Cycles         cycle  4,647,411.65
    Total SM Elapsed Cycles          cycle   523,486,904
    Average SMSP Active Cycles       cycle  2,759,826.02
    Total SMSP Elapsed Cycles        cycle 2,093,947,616
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.59%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.19% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.482%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 13.91% above the average, while the minimum instance value is 19.89% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,853,996
    Memory Throughput                 %        12.38
    DRAM Throughput                   %         0.04
    Duration                         ms         6.35
    L1/TEX Cache Throughput           %        12.92
    L2 Cache Throughput               %         2.93
    SM Active Cycles              cycle 4,641,689.10
    Compute (SM) Throughput           %        10.26
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.70
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.67%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,063.90
    Total DRAM Elapsed Cycles        cycle   308,370,432
    Average L1 Active Cycles         cycle  4,641,689.10
    Total L1 Elapsed Cycles          cycle   522,919,244
    Average L2 Active Cycles         cycle  2,433,497.75
    Total L2 Elapsed Cycles          cycle   365,476,560
    Average SM Active Cycles         cycle  4,641,689.10
    Total SM Elapsed Cycles          cycle   522,919,244
    Average SMSP Active Cycles       cycle  2,756,905.63
    Total SMSP Elapsed Cycles        cycle 2,091,676,976
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.58%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.18% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.784%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 16.49% above the average, while the minimum instance value is 25.09% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,840,828
    Memory Throughput                 %        12.35
    DRAM Throughput                   %         0.04
    Duration                         ms         6.33
    L1/TEX Cache Throughput           %        12.93
    L2 Cache Throughput               %         2.90
    SM Active Cycles              cycle 4,635,995.06
    Compute (SM) Throughput           %        10.23
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.62%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,433.20
    Total DRAM Elapsed Cycles        cycle   307,533,824
    Average L1 Active Cycles         cycle  4,635,995.06
    Total L1 Elapsed Cycles          cycle   524,585,254
    Average L2 Active Cycles         cycle  2,471,121.08
    Total L2 Elapsed Cycles          cycle   364,485,280
    Average SM Active Cycles         cycle  4,635,995.06
    Total SM Elapsed Cycles          cycle   524,585,254
    Average SMSP Active Cycles       cycle  2,750,940.96
    Total SMSP Elapsed Cycles        cycle 2,098,341,016
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.35%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.00% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.75%                                                                                           
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.29% above the average, while the minimum instance value is 18.92% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,887,646
    Memory Throughput                 %        12.39
    DRAM Throughput                   %         0.04
    Duration                         ms         6.39
    L1/TEX Cache Throughput           %        12.93
    L2 Cache Throughput               %         2.88
    SM Active Cycles              cycle 4,638,858.69
    Compute (SM) Throughput           %        10.26
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.62%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,865.80
    Total DRAM Elapsed Cycles        cycle   310,509,568
    Average L1 Active Cycles         cycle  4,638,858.69
    Total L1 Elapsed Cycles          cycle   522,864,416
    Average L2 Active Cycles         cycle  2,442,126.96
    Total L2 Elapsed Cycles          cycle   368,010,400
    Average SM Active Cycles         cycle  4,638,858.69
    Total SM Elapsed Cycles          cycle   522,864,416
    Average SMSP Active Cycles       cycle  2,751,065.07
    Total SMSP Elapsed Cycles        cycle 2,091,457,664
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.63%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.35% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.056%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 13.29% above the average, while the minimum instance value is 20.73% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,873,375
    Memory Throughput                 %        12.35
    DRAM Throughput                   %         0.04
    Duration                         ms         6.37
    L1/TEX Cache Throughput           %        12.93
    L2 Cache Throughput               %         2.91
    SM Active Cycles              cycle 4,636,890.60
    Compute (SM) Throughput           %        10.23
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.63%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,260.60
    Total DRAM Elapsed Cycles        cycle   309,602,304
    Average L1 Active Cycles         cycle  4,636,890.60
    Total L1 Elapsed Cycles          cycle   524,544,026
    Average L2 Active Cycles         cycle  2,449,196.80
    Total L2 Elapsed Cycles          cycle   366,935,760
    Average SM Active Cycles         cycle  4,636,890.60
    Total SM Elapsed Cycles          cycle   524,544,026
    Average SMSP Active Cycles       cycle  2,749,840.64
    Total SMSP Elapsed Cycles        cycle 2,098,176,104
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.75%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.72% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.096%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 11.42% above the average, while the minimum instance value is 19.94% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,873,825
    Memory Throughput                 %        12.38
    DRAM Throughput                   %         0.04
    Duration                         ms         6.37
    L1/TEX Cache Throughput           %        12.91
    L2 Cache Throughput               %         2.90
    SM Active Cycles              cycle 4,643,952.89
    Compute (SM) Throughput           %        10.26
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.62%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,397.30
    Total DRAM Elapsed Cycles        cycle   309,630,976
    Average L1 Active Cycles         cycle  4,643,952.89
    Total L1 Elapsed Cycles          cycle   523,111,346
    Average L2 Active Cycles         cycle  2,483,548.30
    Total L2 Elapsed Cycles          cycle   366,969,520
    Average SM Active Cycles         cycle  4,643,952.89
    Total SM Elapsed Cycles          cycle   523,111,346
    Average SMSP Active Cycles       cycle  2,754,319.86
    Total SMSP Elapsed Cycles        cycle 2,092,445,384
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.68%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.40% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.203%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 11.46% above the average, while the minimum instance value is 19.94% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       765.00
    Elapsed Cycles                cycle    4,834,424
    Memory Throughput                 %        12.37
    DRAM Throughput                   %         0.04
    Duration                         ms         6.32
    L1/TEX Cache Throughput           %        12.95
    L2 Cache Throughput               %         2.90
    SM Active Cycles              cycle 4,630,679.93
    Compute (SM) Throughput           %        10.25
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.58%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,303.20
    Total DRAM Elapsed Cycles        cycle   307,125,504
    Average L1 Active Cycles         cycle  4,630,679.93
    Total L1 Elapsed Cycles          cycle   523,497,094
    Average L2 Active Cycles         cycle  2,462,929.50
    Total L2 Elapsed Cycles          cycle   364,002,960
    Average SM Active Cycles         cycle  4,630,679.93
    Total SM Elapsed Cycles          cycle   523,497,094
    Average SMSP Active Cycles       cycle  2,745,876.18
    Total SMSP Elapsed Cycles        cycle 2,093,988,376
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.34%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 42.97% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.131%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 11.33% above the average, while the minimum instance value is 16.61% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,851,187
    Memory Throughput                 %        12.28
    DRAM Throughput                   %         0.05
    Duration                         ms         6.34
    L1/TEX Cache Throughput           %        12.92
    L2 Cache Throughput               %         2.90
    SM Active Cycles              cycle 4,642,897.60
    Compute (SM) Throughput           %        10.18
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.61%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,533.50
    Total DRAM Elapsed Cycles        cycle   308,192,768
    Average L1 Active Cycles         cycle  4,642,897.60
    Total L1 Elapsed Cycles          cycle   527,358,602
    Average L2 Active Cycles         cycle  2,461,483.30
    Total L2 Elapsed Cycles          cycle   365,265,280
    Average SM Active Cycles         cycle  4,642,897.60
    Total SM Elapsed Cycles          cycle   527,358,602
    Average SMSP Active Cycles       cycle  2,751,875.88
    Total SMSP Elapsed Cycles        cycle 2,109,434,408
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.35%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.20% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.63%                                                                                           
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 14.15% above the average, while the minimum instance value is 16.08% below  
          the average.                                                                                                  

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,854,081
    Memory Throughput                 %        12.32
    DRAM Throughput                   %         0.04
    Duration                         ms         6.35
    L1/TEX Cache Throughput           %        12.92
    L2 Cache Throughput               %         2.91
    SM Active Cycles              cycle 4,640,105.66
    Compute (SM) Throughput           %        10.21
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.62%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,076.40
    Total DRAM Elapsed Cycles        cycle   308,376,576
    Average L1 Active Cycles         cycle  4,640,105.66
    Total L1 Elapsed Cycles          cycle   525,574,814
    Average L2 Active Cycles         cycle  2,461,923.54
    Total L2 Elapsed Cycles          cycle   365,483,520
    Average SM Active Cycles         cycle  4,640,105.66
    Total SM Elapsed Cycles          cycle   525,574,814
    Average SMSP Active Cycles       cycle  2,751,976.17
    Total SMSP Elapsed Cycles        cycle 2,102,299,256
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.43%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.20% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.169%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 13.30% above the average, while the minimum instance value is 19.67% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,851,377
    Memory Throughput                 %        12.30
    DRAM Throughput                   %         0.05
    Duration                         ms         6.34
    L1/TEX Cache Throughput           %        12.93
    L2 Cache Throughput               %         2.92
    SM Active Cycles              cycle 4,636,463.67
    Compute (SM) Throughput           %        10.19
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.72
    Achieved Active Warps Per SM           warp         2.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.55%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,858.60
    Total DRAM Elapsed Cycles        cycle   308,205,056
    Average L1 Active Cycles         cycle  4,636,463.67
    Total L1 Elapsed Cycles          cycle   526,627,748
    Average L2 Active Cycles         cycle  2,464,720.52
    Total L2 Elapsed Cycles          cycle   365,279,680
    Average SM Active Cycles         cycle  4,636,463.67
    Total SM Elapsed Cycles          cycle   526,627,748
    Average SMSP Active Cycles       cycle  2,751,803.19
    Total SMSP Elapsed Cycles        cycle 2,106,510,992
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.38%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.20% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.063%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.94% above the average, while the minimum instance value is 18.92% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,840,400
    Memory Throughput                 %        12.35
    DRAM Throughput                   %         0.04
    Duration                         ms         6.33
    L1/TEX Cache Throughput           %        12.94
    L2 Cache Throughput               %         2.96
    SM Active Cycles              cycle 4,635,310.21
    Compute (SM) Throughput           %        10.24
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.72
    Achieved Active Warps Per SM           warp         2.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.57%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle         3,020
    Total DRAM Elapsed Cycles        cycle   307,508,224
    Average L1 Active Cycles         cycle  4,635,310.21
    Total L1 Elapsed Cycles          cycle   524,299,904
    Average L2 Active Cycles         cycle  2,449,273.84
    Total L2 Elapsed Cycles          cycle   364,453,440
    Average SM Active Cycles         cycle  4,635,310.21
    Total SM Elapsed Cycles          cycle   524,299,904
    Average SMSP Active Cycles       cycle  2,752,939.80
    Total SMSP Elapsed Cycles        cycle 2,097,199,616
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.52%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.24% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.224%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.30% above the average, while the minimum instance value is 23.68% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,878,458
    Memory Throughput                 %        12.35
    DRAM Throughput                   %         0.04
    Duration                         ms         6.38
    L1/TEX Cache Throughput           %        12.93
    L2 Cache Throughput               %         2.90
    SM Active Cycles              cycle 4,637,214.51
    Compute (SM) Throughput           %        10.23
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.59%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,285.20
    Total DRAM Elapsed Cycles        cycle   309,925,632
    Average L1 Active Cycles         cycle  4,637,214.51
    Total L1 Elapsed Cycles          cycle   524,385,292
    Average L2 Active Cycles         cycle  2,468,802.23
    Total L2 Elapsed Cycles          cycle   367,318,960
    Average SM Active Cycles         cycle  4,637,214.51
    Total SM Elapsed Cycles          cycle   524,385,292
    Average SMSP Active Cycles       cycle  2,751,803.40
    Total SMSP Elapsed Cycles        cycle 2,097,541,168
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.67%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.52% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.342%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 11.80% above the average, while the minimum instance value is 19.03% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,832,162
    Memory Throughput                 %        12.40
    DRAM Throughput                   %         0.05
    Duration                         ms         6.32
    L1/TEX Cache Throughput           %        12.92
    L2 Cache Throughput               %         2.93
    SM Active Cycles              cycle 4,640,783.74
    Compute (SM) Throughput           %        10.28
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.70
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.67%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,498.30
    Total DRAM Elapsed Cycles        cycle   306,983,680
    Average L1 Active Cycles         cycle  4,640,783.74
    Total L1 Elapsed Cycles          cycle   522,281,138
    Average L2 Active Cycles         cycle  2,449,771.09
    Total L2 Elapsed Cycles          cycle   363,832,320
    Average SM Active Cycles         cycle  4,640,783.74
    Total SM Elapsed Cycles          cycle   522,281,138
    Average SMSP Active Cycles       cycle  2,754,098.10
    Total SMSP Elapsed Cycles        cycle 2,089,124,552
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.63%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.26% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.915%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 10.98% above the average, while the minimum instance value is 19.16% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,854,586
    Memory Throughput                 %        12.36
    DRAM Throughput                   %         0.04
    Duration                         ms         6.35
    L1/TEX Cache Throughput           %        12.94
    L2 Cache Throughput               %         2.95
    SM Active Cycles              cycle 4,633,191.89
    Compute (SM) Throughput           %        10.24
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.72
    Achieved Active Warps Per SM           warp         2.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.54%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle         3,334
    Total DRAM Elapsed Cycles        cycle   308,408,320
    Average L1 Active Cycles         cycle  4,633,191.89
    Total L1 Elapsed Cycles          cycle   524,022,302
    Average L2 Active Cycles         cycle  2,446,270.62
    Total L2 Elapsed Cycles          cycle   365,520,800
    Average SM Active Cycles         cycle  4,633,191.89
    Total SM Elapsed Cycles          cycle   524,022,302
    Average SMSP Active Cycles       cycle  2,748,146.87
    Total SMSP Elapsed Cycles        cycle 2,096,089,208
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.58%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.40% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.411%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 11.97% above the average, while the minimum instance value is 21.36% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,850,033
    Memory Throughput                 %        12.39
    DRAM Throughput                   %         0.04
    Duration                         ms         6.34
    L1/TEX Cache Throughput           %        12.91
    L2 Cache Throughput               %         2.91
    SM Active Cycles              cycle 4,643,465.16
    Compute (SM) Throughput           %        10.27
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.6%                                                                                     
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,318.80
    Total DRAM Elapsed Cycles        cycle   308,119,040
    Average L1 Active Cycles         cycle  4,643,465.16
    Total L1 Elapsed Cycles          cycle   522,674,564
    Average L2 Active Cycles         cycle  2,459,991.95
    Total L2 Elapsed Cycles          cycle   365,178,240
    Average SM Active Cycles         cycle  4,643,465.16
    Total SM Elapsed Cycles          cycle   522,674,564
    Average SMSP Active Cycles       cycle  2,755,588.07
    Total SMSP Elapsed Cycles        cycle 2,090,698,256
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.58%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.17% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.48%                                                                                           
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 12.03% above the average, while the minimum instance value is 15.83% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,841,286
    Memory Throughput                 %        12.35
    DRAM Throughput                   %         0.04
    Duration                         ms         6.33
    L1/TEX Cache Throughput           %        12.90
    L2 Cache Throughput               %         2.96
    SM Active Cycles              cycle 4,647,551.55
    Compute (SM) Throughput           %        10.24
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.70
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.71%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,256.80
    Total DRAM Elapsed Cycles        cycle   307,563,520
    Average L1 Active Cycles         cycle  4,647,551.55
    Total L1 Elapsed Cycles          cycle   524,274,986
    Average L2 Active Cycles         cycle  2,471,365.45
    Total L2 Elapsed Cycles          cycle   364,519,600
    Average SM Active Cycles         cycle  4,647,551.55
    Total SM Elapsed Cycles          cycle   524,274,986
    Average SMSP Active Cycles       cycle  2,759,508.04
    Total SMSP Elapsed Cycles        cycle 2,097,099,944
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.5%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.10% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.29%                                                                                           
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 11.60% above the average, while the minimum instance value is 18.71% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,835,909
    Memory Throughput                 %        12.41
    DRAM Throughput                   %         0.05
    Duration                         ms         6.32
    L1/TEX Cache Throughput           %        12.93
    L2 Cache Throughput               %         2.90
    SM Active Cycles              cycle 4,637,153.88
    Compute (SM) Throughput           %        10.28
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.62%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,456.40
    Total DRAM Elapsed Cycles        cycle   307,222,528
    Average L1 Active Cycles         cycle  4,637,153.88
    Total L1 Elapsed Cycles          cycle   522,005,916
    Average L2 Active Cycles         cycle  2,491,329.08
    Total L2 Elapsed Cycles          cycle   364,115,120
    Average SM Active Cycles         cycle  4,637,153.88
    Total SM Elapsed Cycles          cycle   522,005,916
    Average SMSP Active Cycles       cycle  2,753,909.22
    Total SMSP Elapsed Cycles        cycle 2,088,023,664
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.53%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.06% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.789%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 12.40% above the average, while the minimum instance value is 20.62% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,885,409
    Memory Throughput                 %        12.37
    DRAM Throughput                   %         0.04
    Duration                         ms         6.39
    L1/TEX Cache Throughput           %        12.93
    L2 Cache Throughput               %         2.94
    SM Active Cycles              cycle 4,638,233.13
    Compute (SM) Throughput           %        10.25
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.64%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,089.80
    Total DRAM Elapsed Cycles        cycle   310,366,720
    Average L1 Active Cycles         cycle  4,638,233.13
    Total L1 Elapsed Cycles          cycle   523,614,012
    Average L2 Active Cycles         cycle  2,440,324.58
    Total L2 Elapsed Cycles          cycle   367,842,480
    Average SM Active Cycles         cycle  4,638,233.13
    Total SM Elapsed Cycles          cycle   523,614,012
    Average SMSP Active Cycles       cycle  2,755,663.59
    Total SMSP Elapsed Cycles        cycle 2,094,456,048
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.67%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.41% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.198%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 13.56% above the average, while the minimum instance value is 20.85% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,868,717
    Memory Throughput                 %        12.37
    DRAM Throughput                   %         0.04
    Duration                         ms         6.36
    L1/TEX Cache Throughput           %        12.93
    L2 Cache Throughput               %         2.93
    SM Active Cycles              cycle 4,639,017.10
    Compute (SM) Throughput           %        10.25
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.64%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,948.90
    Total DRAM Elapsed Cycles        cycle   309,307,008
    Average L1 Active Cycles         cycle  4,639,017.10
    Total L1 Elapsed Cycles          cycle   523,737,130
    Average L2 Active Cycles         cycle  2,430,555.88
    Total L2 Elapsed Cycles          cycle   366,585,680
    Average SM Active Cycles         cycle  4,639,017.10
    Total SM Elapsed Cycles          cycle   523,737,130
    Average SMSP Active Cycles       cycle  2,758,336.13
    Total SMSP Elapsed Cycles        cycle 2,094,948,520
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.7%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.43% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.683%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 10.71% above the average, while the minimum instance value is 18.75% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,855,550
    Memory Throughput                 %        12.36
    DRAM Throughput                   %         0.04
    Duration                         ms         6.35
    L1/TEX Cache Throughput           %        12.91
    L2 Cache Throughput               %         2.91
    SM Active Cycles              cycle 4,643,779.54
    Compute (SM) Throughput           %        10.25
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.70
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.65%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,230.70
    Total DRAM Elapsed Cycles        cycle   308,469,760
    Average L1 Active Cycles         cycle  4,643,779.54
    Total L1 Elapsed Cycles          cycle   523,762,928
    Average L2 Active Cycles         cycle  2,481,656.04
    Total L2 Elapsed Cycles          cycle   365,593,520
    Average SM Active Cycles         cycle  4,643,779.54
    Total SM Elapsed Cycles          cycle   523,762,928
    Average SMSP Active Cycles       cycle  2,757,138.68
    Total SMSP Elapsed Cycles        cycle 2,095,051,712
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.6%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.27% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.914%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.57% above the average, while the minimum instance value is 22.98% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,874,947
    Memory Throughput                 %        12.35
    DRAM Throughput                   %         0.05
    Duration                         ms         6.37
    L1/TEX Cache Throughput           %        12.90
    L2 Cache Throughput               %         2.90
    SM Active Cycles              cycle 4,646,761.29
    Compute (SM) Throughput           %        10.24
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.70
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.71%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,627.90
    Total DRAM Elapsed Cycles        cycle   309,702,144
    Average L1 Active Cycles         cycle  4,646,761.29
    Total L1 Elapsed Cycles          cycle   524,211,528
    Average L2 Active Cycles         cycle  2,483,216.84
    Total L2 Elapsed Cycles          cycle   367,054,720
    Average SM Active Cycles         cycle  4,646,761.29
    Total SM Elapsed Cycles          cycle   524,211,528
    Average SMSP Active Cycles       cycle  2,757,189.17
    Total SMSP Elapsed Cycles        cycle 2,096,846,112
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.72%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.51% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.288%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 13.47% above the average, while the minimum instance value is 21.49% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,862,811
    Memory Throughput                 %        12.42
    DRAM Throughput                   %         0.04
    Duration                         ms         6.36
    L1/TEX Cache Throughput           %        12.91
    L2 Cache Throughput               %         2.92
    SM Active Cycles              cycle 4,643,475.22
    Compute (SM) Throughput           %        10.29
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.64%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,255.50
    Total DRAM Elapsed Cycles        cycle   308,931,072
    Average L1 Active Cycles         cycle  4,643,475.22
    Total L1 Elapsed Cycles          cycle   521,383,050
    Average L2 Active Cycles         cycle  2,456,796.25
    Total L2 Elapsed Cycles          cycle   366,141,040
    Average SM Active Cycles         cycle  4,643,475.22
    Total SM Elapsed Cycles          cycle   521,383,050
    Average SMSP Active Cycles       cycle  2,755,877.81
    Total SMSP Elapsed Cycles        cycle 2,085,532,200
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.87%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.57% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.425%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 13.83% above the average, while the minimum instance value is 21.84% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,878,427
    Memory Throughput                 %        12.35
    DRAM Throughput                   %         0.05
    Duration                         ms         6.38
    L1/TEX Cache Throughput           %        12.90
    L2 Cache Throughput               %         2.92
    SM Active Cycles              cycle 4,647,203.45
    Compute (SM) Throughput           %        10.24
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.70
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.66%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,514.40
    Total DRAM Elapsed Cycles        cycle   309,922,560
    Average L1 Active Cycles         cycle  4,647,203.45
    Total L1 Elapsed Cycles          cycle   524,282,132
    Average L2 Active Cycles         cycle  2,462,759.17
    Total L2 Elapsed Cycles          cycle   367,316,000
    Average SM Active Cycles         cycle  4,647,203.45
    Total SM Elapsed Cycles          cycle   524,282,132
    Average SMSP Active Cycles       cycle  2,757,412.99
    Total SMSP Elapsed Cycles        cycle 2,097,128,528
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.68%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.45% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.773%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 10.76% above the average, while the minimum instance value is 17.45% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,876,857
    Memory Throughput                 %        12.36
    DRAM Throughput                   %         0.04
    Duration                         ms         6.38
    L1/TEX Cache Throughput           %        12.92
    L2 Cache Throughput               %         2.90
    SM Active Cycles              cycle 4,639,898.59
    Compute (SM) Throughput           %        10.25
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.63%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,038.10
    Total DRAM Elapsed Cycles        cycle   309,823,104
    Average L1 Active Cycles         cycle  4,639,898.59
    Total L1 Elapsed Cycles          cycle   523,828,638
    Average L2 Active Cycles         cycle  2,462,304.50
    Total L2 Elapsed Cycles          cycle   367,198,560
    Average SM Active Cycles         cycle  4,639,898.59
    Total SM Elapsed Cycles          cycle   523,828,638
    Average SMSP Active Cycles       cycle  2,749,669.64
    Total SMSP Elapsed Cycles        cycle 2,095,314,552
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.65%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.49% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.913%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 11.02% above the average, while the minimum instance value is 16.65% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,878,387
    Memory Throughput                 %        12.39
    DRAM Throughput                   %         0.04
    Duration                         ms         6.38
    L1/TEX Cache Throughput           %        12.91
    L2 Cache Throughput               %         2.93
    SM Active Cycles              cycle 4,643,898.86
    Compute (SM) Throughput           %        10.27
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.63%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,315.90
    Total DRAM Elapsed Cycles        cycle   309,920,768
    Average L1 Active Cycles         cycle  4,643,898.86
    Total L1 Elapsed Cycles          cycle   522,622,178
    Average L2 Active Cycles         cycle  2,433,196.79
    Total L2 Elapsed Cycles          cycle   367,313,200
    Average SM Active Cycles         cycle  4,643,898.86
    Total SM Elapsed Cycles          cycle   522,622,178
    Average SMSP Active Cycles       cycle  2,756,508.45
    Total SMSP Elapsed Cycles        cycle 2,090,488,712
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.62%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.22% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.912%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 13.04% above the average, while the minimum instance value is 19.21% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,842,869
    Memory Throughput                 %        12.36
    DRAM Throughput                   %         0.04
    Duration                         ms         6.33
    L1/TEX Cache Throughput           %        12.91
    L2 Cache Throughput               %         2.92
    SM Active Cycles              cycle 4,645,623.03
    Compute (SM) Throughput           %        10.25
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.65%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,893.50
    Total DRAM Elapsed Cycles        cycle   307,663,872
    Average L1 Active Cycles         cycle  4,645,623.03
    Total L1 Elapsed Cycles          cycle   523,786,814
    Average L2 Active Cycles         cycle  2,472,372.58
    Total L2 Elapsed Cycles          cycle   364,639,040
    Average SM Active Cycles         cycle  4,645,623.03
    Total SM Elapsed Cycles          cycle   523,786,814
    Average SMSP Active Cycles       cycle  2,755,831.29
    Total SMSP Elapsed Cycles        cycle 2,095,147,256
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.54%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.19% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.21%                                                                                           
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 13.29% above the average, while the minimum instance value is 14.19% below  
          the average.                                                                                                  

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,858,655
    Memory Throughput                 %        12.35
    DRAM Throughput                   %         0.05
    Duration                         ms         6.35
    L1/TEX Cache Throughput           %        12.88
    L2 Cache Throughput               %         2.97
    SM Active Cycles              cycle 4,654,342.67
    Compute (SM) Throughput           %        10.24
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.63%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,582.90
    Total DRAM Elapsed Cycles        cycle   308,666,880
    Average L1 Active Cycles         cycle  4,654,342.67
    Total L1 Elapsed Cycles          cycle   524,282,234
    Average L2 Active Cycles         cycle  2,441,105.94
    Total L2 Elapsed Cycles          cycle   365,827,280
    Average SM Active Cycles         cycle  4,654,342.67
    Total SM Elapsed Cycles          cycle   524,282,234
    Average SMSP Active Cycles       cycle  2,767,052.87
    Total SMSP Elapsed Cycles        cycle 2,097,128,936
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.43%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 42.85% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.576%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 12.32% above the average, while the minimum instance value is 22.54% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,856,943
    Memory Throughput                 %        12.40
    DRAM Throughput                   %         0.04
    Duration                         ms         6.35
    L1/TEX Cache Throughput           %        12.92
    L2 Cache Throughput               %         2.92
    SM Active Cycles              cycle 4,642,108.48
    Compute (SM) Throughput           %        10.27
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.64%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,259.40
    Total DRAM Elapsed Cycles        cycle   308,558,336
    Average L1 Active Cycles         cycle  4,642,108.48
    Total L1 Elapsed Cycles          cycle   522,415,964
    Average L2 Active Cycles         cycle  2,453,178.08
    Total L2 Elapsed Cycles          cycle   365,699,200
    Average SM Active Cycles         cycle  4,642,108.48
    Total SM Elapsed Cycles          cycle   522,415,964
    Average SMSP Active Cycles       cycle  2,756,653.22
    Total SMSP Elapsed Cycles        cycle 2,089,663,856
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.53%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.05% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.476%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 13.93% above the average, while the minimum instance value is 19.55% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,836,738
    Memory Throughput                 %        12.35
    DRAM Throughput                   %         0.05
    Duration                         ms         6.32
    L1/TEX Cache Throughput           %        12.93
    L2 Cache Throughput               %         2.96
    SM Active Cycles              cycle 4,639,025.31
    Compute (SM) Throughput           %        10.23
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.6%                                                                                     
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle         3,477
    Total DRAM Elapsed Cycles        cycle   307,274,752
    Average L1 Active Cycles         cycle  4,639,025.31
    Total L1 Elapsed Cycles          cycle   524,376,584
    Average L2 Active Cycles         cycle  2,463,822.31
    Total L2 Elapsed Cycles          cycle   364,177,840
    Average SM Active Cycles         cycle  4,639,025.31
    Total SM Elapsed Cycles          cycle   524,376,584
    Average SMSP Active Cycles       cycle  2,757,625.43
    Total SMSP Elapsed Cycles        cycle 2,097,506,336
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.43%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.01% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.468%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 13.80% above the average, while the minimum instance value is 24.56% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,851,241
    Memory Throughput                 %        12.40
    DRAM Throughput                   %         0.05
    Duration                         ms         6.34
    L1/TEX Cache Throughput           %        12.92
    L2 Cache Throughput               %         2.87
    SM Active Cycles              cycle 4,640,855.86
    Compute (SM) Throughput           %        10.27
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.70
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.66%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,573.80
    Total DRAM Elapsed Cycles        cycle   308,196,352
    Average L1 Active Cycles         cycle  4,640,855.86
    Total L1 Elapsed Cycles          cycle   522,322,280
    Average L2 Active Cycles         cycle  2,505,809.41
    Total L2 Elapsed Cycles          cycle   365,269,520
    Average SM Active Cycles         cycle  4,640,855.86
    Total SM Elapsed Cycles          cycle   522,322,280
    Average SMSP Active Cycles       cycle  2,753,681.94
    Total SMSP Elapsed Cycles        cycle 2,089,289,120
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.58%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.16% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.953%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 12.67% above the average, while the minimum instance value is 21.85% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,855,387
    Memory Throughput                 %        12.34
    DRAM Throughput                   %         0.04
    Duration                         ms         6.35
    L1/TEX Cache Throughput           %        12.92
    L2 Cache Throughput               %         2.92
    SM Active Cycles              cycle 4,641,944.24
    Compute (SM) Throughput           %        10.23
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.70
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.66%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,090.10
    Total DRAM Elapsed Cycles        cycle   308,459,008
    Average L1 Active Cycles         cycle  4,641,944.24
    Total L1 Elapsed Cycles          cycle   524,699,546
    Average L2 Active Cycles         cycle  2,441,493.44
    Total L2 Elapsed Cycles          cycle   365,581,120
    Average SM Active Cycles         cycle  4,641,944.24
    Total SM Elapsed Cycles          cycle   524,699,546
    Average SMSP Active Cycles       cycle  2,756,897.52
    Total SMSP Elapsed Cycles        cycle 2,098,798,184
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.49%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.16% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.103%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 13.29% above the average, while the minimum instance value is 18.69% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,853,301
    Memory Throughput                 %        12.45
    DRAM Throughput                   %         0.05
    Duration                         ms         6.34
    L1/TEX Cache Throughput           %        12.93
    L2 Cache Throughput               %         2.94
    SM Active Cycles              cycle 4,638,118.50
    Compute (SM) Throughput           %        10.32
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.64%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,588.50
    Total DRAM Elapsed Cycles        cycle   308,327,424
    Average L1 Active Cycles         cycle  4,638,118.50
    Total L1 Elapsed Cycles          cycle   520,256,492
    Average L2 Active Cycles         cycle  2,428,538.61
    Total L2 Elapsed Cycles          cycle   365,424,960
    Average SM Active Cycles         cycle  4,638,118.50
    Total SM Elapsed Cycles          cycle   520,256,492
    Average SMSP Active Cycles       cycle  2,754,304.78
    Total SMSP Elapsed Cycles        cycle 2,081,025,968
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.69%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.18% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.881%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 12.94% above the average, while the minimum instance value is 20.73% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,835,251
    Memory Throughput                 %        12.37
    DRAM Throughput                   %         0.04
    Duration                         ms         6.32
    L1/TEX Cache Throughput           %        12.90
    L2 Cache Throughput               %         2.93
    SM Active Cycles              cycle 4,647,535.69
    Compute (SM) Throughput           %        10.25
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.70
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.67%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,421.20
    Total DRAM Elapsed Cycles        cycle   307,180,544
    Average L1 Active Cycles         cycle  4,647,535.69
    Total L1 Elapsed Cycles          cycle   523,667,300
    Average L2 Active Cycles         cycle  2,462,574.52
    Total L2 Elapsed Cycles          cycle   364,065,920
    Average SM Active Cycles         cycle  4,647,535.69
    Total SM Elapsed Cycles          cycle   523,667,300
    Average SMSP Active Cycles       cycle  2,759,091.05
    Total SMSP Elapsed Cycles        cycle 2,094,669,200
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.37%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 42.83% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.299%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 13.49% above the average, while the minimum instance value is 24.47% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,856,799
    Memory Throughput                 %        12.36
    DRAM Throughput                   %         0.04
    Duration                         ms         6.35
    L1/TEX Cache Throughput           %        12.93
    L2 Cache Throughput               %         2.93
    SM Active Cycles              cycle 4,638,212.69
    Compute (SM) Throughput           %        10.24
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.6%                                                                                     
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,363.50
    Total DRAM Elapsed Cycles        cycle   308,550,144
    Average L1 Active Cycles         cycle  4,638,212.69
    Total L1 Elapsed Cycles          cycle   523,891,632
    Average L2 Active Cycles         cycle  2,428,463.34
    Total L2 Elapsed Cycles          cycle   365,688,320
    Average SM Active Cycles         cycle  4,638,212.69
    Total SM Elapsed Cycles          cycle   523,891,632
    Average SMSP Active Cycles       cycle  2,754,598.16
    Total SMSP Elapsed Cycles        cycle 2,095,566,528
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.53%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.20% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.541%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.19% above the average, while the minimum instance value is 19.93% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,839,023
    Memory Throughput                 %        12.38
    DRAM Throughput                   %         0.04
    Duration                         ms         6.33
    L1/TEX Cache Throughput           %        12.94
    L2 Cache Throughput               %         2.93
    SM Active Cycles              cycle 4,635,902.60
    Compute (SM) Throughput           %        10.26
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.72
    Achieved Active Warps Per SM           warp         2.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.57%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,965.20
    Total DRAM Elapsed Cycles        cycle   307,420,160
    Average L1 Active Cycles         cycle  4,635,902.60
    Total L1 Elapsed Cycles          cycle   523,326,034
    Average L2 Active Cycles         cycle  2,459,226.06
    Total L2 Elapsed Cycles          cycle   364,349,760
    Average SM Active Cycles         cycle  4,635,902.60
    Total SM Elapsed Cycles          cycle   523,326,034
    Average SMSP Active Cycles       cycle  2,754,858.59
    Total SMSP Elapsed Cycles        cycle 2,093,304,136
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.44%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.00% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.738%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 12.48% above the average, while the minimum instance value is 22.28% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,872,472
    Memory Throughput                 %        12.37
    DRAM Throughput                   %         0.04
    Duration                         ms         6.37
    L1/TEX Cache Throughput           %        12.92
    L2 Cache Throughput               %         2.92
    SM Active Cycles              cycle 4,640,012.40
    Compute (SM) Throughput           %        10.26
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.72
    Achieved Active Warps Per SM           warp         2.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.56%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,355.40
    Total DRAM Elapsed Cycles        cycle   309,544,960
    Average L1 Active Cycles         cycle  4,640,012.40
    Total L1 Elapsed Cycles          cycle   523,267,612
    Average L2 Active Cycles         cycle  2,444,149.59
    Total L2 Elapsed Cycles          cycle   366,868,160
    Average SM Active Cycles         cycle  4,640,012.40
    Total SM Elapsed Cycles          cycle   523,267,612
    Average SMSP Active Cycles       cycle  2,750,887.05
    Total SMSP Elapsed Cycles        cycle 2,093,070,448
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.69%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.49% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.443%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 13.97% above the average, while the minimum instance value is 15.23% below  
          the average.                                                                                                  

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,879,479
    Memory Throughput                 %        12.40
    DRAM Throughput                   %         0.04
    Duration                         ms         6.38
    L1/TEX Cache Throughput           %        12.93
    L2 Cache Throughput               %         2.92
    SM Active Cycles              cycle 4,638,925.69
    Compute (SM) Throughput           %        10.28
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.72
    Achieved Active Warps Per SM           warp         2.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.58%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,326.40
    Total DRAM Elapsed Cycles        cycle   309,990,400
    Average L1 Active Cycles         cycle  4,638,925.69
    Total L1 Elapsed Cycles          cycle   522,050,952
    Average L2 Active Cycles         cycle  2,473,826.49
    Total L2 Elapsed Cycles          cycle   367,395,920
    Average SM Active Cycles         cycle  4,638,925.69
    Total SM Elapsed Cycles          cycle   522,050,952
    Average SMSP Active Cycles       cycle  2,751,507.77
    Total SMSP Elapsed Cycles        cycle 2,088,203,808
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.75%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.47% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.707%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 12.45% above the average, while the minimum instance value is 16.21% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,891,468
    Memory Throughput                 %        12.41
    DRAM Throughput                   %         0.05
    Duration                         ms         6.39
    L1/TEX Cache Throughput           %        12.91
    L2 Cache Throughput               %         2.92
    SM Active Cycles              cycle 4,646,297.78
    Compute (SM) Throughput           %        10.29
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.62%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,564.10
    Total DRAM Elapsed Cycles        cycle   310,751,744
    Average L1 Active Cycles         cycle  4,646,297.78
    Total L1 Elapsed Cycles          cycle   521,765,434
    Average L2 Active Cycles         cycle  2,421,290.84
    Total L2 Elapsed Cycles          cycle   368,298,800
    Average SM Active Cycles         cycle  4,646,297.78
    Total SM Elapsed Cycles          cycle   521,765,434
    Average SMSP Active Cycles       cycle  2,757,821.47
    Total SMSP Elapsed Cycles        cycle 2,087,061,736
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.69%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.25% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.13%                                                                                           
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.46% above the average, while the minimum instance value is 19.71% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,883,303
    Memory Throughput                 %        12.37
    DRAM Throughput                   %         0.04
    Duration                         ms         6.38
    L1/TEX Cache Throughput           %        12.93
    L2 Cache Throughput               %         2.91
    SM Active Cycles              cycle 4,637,202.76
    Compute (SM) Throughput           %        10.25
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.61%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,199.30
    Total DRAM Elapsed Cycles        cycle   310,233,088
    Average L1 Active Cycles         cycle  4,637,202.76
    Total L1 Elapsed Cycles          cycle   523,417,068
    Average L2 Active Cycles         cycle  2,427,975.20
    Total L2 Elapsed Cycles          cycle   367,683,760
    Average SM Active Cycles         cycle  4,637,202.76
    Total SM Elapsed Cycles          cycle   523,417,068
    Average SMSP Active Cycles       cycle  2,754,408.16
    Total SMSP Elapsed Cycles        cycle 2,093,668,272
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.66%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.39% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.165%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 13.56% above the average, while the minimum instance value is 21.47% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,867,388
    Memory Throughput                 %        12.37
    DRAM Throughput                   %         0.04
    Duration                         ms         6.36
    L1/TEX Cache Throughput           %        12.91
    L2 Cache Throughput               %         2.93
    SM Active Cycles              cycle 4,643,972.42
    Compute (SM) Throughput           %        10.25
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.65%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,476.30
    Total DRAM Elapsed Cycles        cycle   309,222,400
    Average L1 Active Cycles         cycle  4,643,972.42
    Total L1 Elapsed Cycles          cycle   523,712,134
    Average L2 Active Cycles         cycle  2,479,896.56
    Total L2 Elapsed Cycles          cycle   366,485,520
    Average SM Active Cycles         cycle  4,643,972.42
    Total SM Elapsed Cycles          cycle   523,712,134
    Average SMSP Active Cycles       cycle  2,757,709.62
    Total SMSP Elapsed Cycles        cycle 2,094,848,536
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.67%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.39% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.282%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 11.60% above the average, while the minimum instance value is 23.81% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,848,220
    Memory Throughput                 %        12.37
    DRAM Throughput                   %         0.05
    Duration                         ms         6.34
    L1/TEX Cache Throughput           %        12.93
    L2 Cache Throughput               %         2.98
    SM Active Cycles              cycle 4,638,936.65
    Compute (SM) Throughput           %        10.25
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.61%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,653.40
    Total DRAM Elapsed Cycles        cycle   308,004,608
    Average L1 Active Cycles         cycle  4,638,936.65
    Total L1 Elapsed Cycles          cycle   523,679,474
    Average L2 Active Cycles         cycle  2,455,959.98
    Total L2 Elapsed Cycles          cycle   365,042,240
    Average SM Active Cycles         cycle  4,638,936.65
    Total SM Elapsed Cycles          cycle   523,679,474
    Average SMSP Active Cycles       cycle  2,753,708.63
    Total SMSP Elapsed Cycles        cycle 2,094,717,896
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.49%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.13% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.863%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.61% above the average, while the minimum instance value is 19.96% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,846,879
    Memory Throughput                 %        12.37
    DRAM Throughput                   %         0.04
    Duration                         ms         6.34
    L1/TEX Cache Throughput           %        12.94
    L2 Cache Throughput               %         2.93
    SM Active Cycles              cycle 4,633,015.56
    Compute (SM) Throughput           %        10.25
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.72
    Achieved Active Warps Per SM           warp         2.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.56%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle         3,342
    Total DRAM Elapsed Cycles        cycle   307,918,848
    Average L1 Active Cycles         cycle  4,633,015.56
    Total L1 Elapsed Cycles          cycle   523,560,322
    Average L2 Active Cycles         cycle  2,436,360.85
    Total L2 Elapsed Cycles          cycle   364,940,720
    Average SM Active Cycles         cycle  4,633,015.56
    Total SM Elapsed Cycles          cycle   523,560,322
    Average SMSP Active Cycles       cycle  2,752,289.10
    Total SMSP Elapsed Cycles        cycle 2,094,241,288
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.55%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.24% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.934%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 12.98% above the average, while the minimum instance value is 17.32% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,849,452
    Memory Throughput                 %        12.34
    DRAM Throughput                   %         0.05
    Duration                         ms         6.34
    L1/TEX Cache Throughput           %        12.93
    L2 Cache Throughput               %         2.95
    SM Active Cycles              cycle 4,637,023.78
    Compute (SM) Throughput           %        10.22
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.59%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,741.60
    Total DRAM Elapsed Cycles        cycle   308,082,688
    Average L1 Active Cycles         cycle  4,637,023.78
    Total L1 Elapsed Cycles          cycle   525,025,862
    Average L2 Active Cycles         cycle  2,434,892.19
    Total L2 Elapsed Cycles          cycle   365,135,120
    Average SM Active Cycles         cycle  4,637,023.78
    Total SM Elapsed Cycles          cycle   525,025,862
    Average SMSP Active Cycles       cycle  2,752,852.92
    Total SMSP Elapsed Cycles        cycle 2,100,103,448
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.44%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.16% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.78%                                                                                           
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 12.71% above the average, while the minimum instance value is 17.05% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,894,894
    Memory Throughput                 %        12.40
    DRAM Throughput                   %         0.04
    Duration                         ms         6.40
    L1/TEX Cache Throughput           %        12.91
    L2 Cache Throughput               %         2.89
    SM Active Cycles              cycle 4,643,819.42
    Compute (SM) Throughput           %        10.28
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.62%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,104.50
    Total DRAM Elapsed Cycles        cycle   310,969,344
    Average L1 Active Cycles         cycle  4,643,819.42
    Total L1 Elapsed Cycles          cycle   522,227,716
    Average L2 Active Cycles         cycle  2,458,988.02
    Total L2 Elapsed Cycles          cycle   368,556,640
    Average SM Active Cycles         cycle  4,643,819.42
    Total SM Elapsed Cycles          cycle   522,227,716
    Average SMSP Active Cycles       cycle  2,755,185.35
    Total SMSP Elapsed Cycles        cycle 2,088,910,864
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.81%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.54% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.379%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.70% above the average, while the minimum instance value is 19.91% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,888,275
    Memory Throughput                 %        12.42
    DRAM Throughput                   %         0.04
    Duration                         ms         6.39
    L1/TEX Cache Throughput           %        12.93
    L2 Cache Throughput               %         2.90
    SM Active Cycles              cycle 4,636,348.74
    Compute (SM) Throughput           %        10.30
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.72
    Achieved Active Warps Per SM           warp         2.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.58%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,395.90
    Total DRAM Elapsed Cycles        cycle   310,548,480
    Average L1 Active Cycles         cycle  4,636,348.74
    Total L1 Elapsed Cycles          cycle   521,232,042
    Average L2 Active Cycles         cycle  2,474,793.05
    Total L2 Elapsed Cycles          cycle   368,057,600
    Average SM Active Cycles         cycle  4,636,348.74
    Total SM Elapsed Cycles          cycle   521,232,042
    Average SMSP Active Cycles       cycle  2,748,613.59
    Total SMSP Elapsed Cycles        cycle 2,084,928,168
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.83%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.59% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.717%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 12.49% above the average, while the minimum instance value is 19.75% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,841,686
    Memory Throughput                 %        12.36
    DRAM Throughput                   %         0.04
    Duration                         ms         6.33
    L1/TEX Cache Throughput           %        12.94
    L2 Cache Throughput               %         2.93
    SM Active Cycles              cycle 4,634,350.60
    Compute (SM) Throughput           %        10.24
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.72
    Achieved Active Warps Per SM           warp         2.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.54%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,357.80
    Total DRAM Elapsed Cycles        cycle   307,589,120
    Average L1 Active Cycles         cycle  4,634,350.60
    Total L1 Elapsed Cycles          cycle   524,189,588
    Average L2 Active Cycles         cycle  2,471,066.12
    Total L2 Elapsed Cycles          cycle   364,550,400
    Average SM Active Cycles         cycle  4,634,350.60
    Total SM Elapsed Cycles          cycle   524,189,588
    Average SMSP Active Cycles       cycle  2,747,876.96
    Total SMSP Elapsed Cycles        cycle 2,096,758,352
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.48%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.24% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.088%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.92% above the average, while the minimum instance value is 21.06% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,850,316
    Memory Throughput                 %        12.38
    DRAM Throughput                   %         0.05
    Duration                         ms         6.34
    L1/TEX Cache Throughput           %        12.94
    L2 Cache Throughput               %         2.95
    SM Active Cycles              cycle 4,633,399.60
    Compute (SM) Throughput           %        10.26
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.72
    Achieved Active Warps Per SM           warp         2.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.58%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,674.10
    Total DRAM Elapsed Cycles        cycle   308,137,984
    Average L1 Active Cycles         cycle  4,633,399.60
    Total L1 Elapsed Cycles          cycle   523,202,088
    Average L2 Active Cycles         cycle  2,462,495.35
    Total L2 Elapsed Cycles          cycle   365,200,240
    Average SM Active Cycles         cycle  4,633,399.60
    Total SM Elapsed Cycles          cycle   523,202,088
    Average SMSP Active Cycles       cycle  2,751,678.87
    Total SMSP Elapsed Cycles        cycle 2,092,808,352
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.54%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.20% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.002%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 9.27% above the average, while the minimum instance value is 22.65% below the       
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,841,911
    Memory Throughput                 %        12.39
    DRAM Throughput                   %         0.05
    Duration                         ms         6.33
    L1/TEX Cache Throughput           %        12.92
    L2 Cache Throughput               %         2.95
    SM Active Cycles              cycle 4,641,723.65
    Compute (SM) Throughput           %        10.26
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.70
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.67%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,501.90
    Total DRAM Elapsed Cycles        cycle   307,602,432
    Average L1 Active Cycles         cycle  4,641,723.65
    Total L1 Elapsed Cycles          cycle   522,869,524
    Average L2 Active Cycles         cycle  2,481,702.33
    Total L2 Elapsed Cycles          cycle   364,566,480
    Average SM Active Cycles         cycle  4,641,723.65
    Total SM Elapsed Cycles          cycle   522,869,524
    Average SMSP Active Cycles       cycle  2,757,704.62
    Total SMSP Elapsed Cycles        cycle 2,091,478,096
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.56%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.12% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.098%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 13.03% above the average, while the minimum instance value is 18.61% below the      
          average.                                                                                                      

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,855,307
    Memory Throughput                 %        12.38
    DRAM Throughput                   %         0.04
    Duration                         ms         6.35
    L1/TEX Cache Throughput           %        12.93
    L2 Cache Throughput               %         2.90
    SM Active Cycles              cycle 4,637,978.33
    Compute (SM) Throughput           %        10.26
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.64%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      3,271.60
    Total DRAM Elapsed Cycles        cycle   308,454,400
    Average L1 Active Cycles         cycle  4,637,978.33
    Total L1 Elapsed Cycles          cycle   523,222,238
    Average L2 Active Cycles         cycle  2,439,847.45
    Total L2 Elapsed Cycles          cycle   365,575,520
    Average SM Active Cycles         cycle  4,637,978.33
    Total SM Elapsed Cycles          cycle   523,222,238
    Average SMSP Active Cycles       cycle  2,755,277.82
    Total SMSP Elapsed Cycles        cycle 2,092,888,952
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.55%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.16% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.603%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 12.37% above the average, while the minimum instance value is 13.88% below  
          the average.                                                                                                  

  flash_attention_forward(const float *, const float *, const float *, float *, float *, float *, int, int, int, int, int) (32, 8, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.21
    SM Frequency                    Mhz       764.99
    Elapsed Cycles                cycle    4,855,625
    Memory Throughput                 %        12.37
    DRAM Throughput                   %         0.04
    Duration                         ms         6.35
    L1/TEX Cache Throughput           %        12.92
    L2 Cache Throughput               %         2.93
    SM Active Cycles              cycle 4,640,558.82
    Compute (SM) Throughput           %        10.25
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           16.51
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Stack Size                                                 1,536
    Threads                                   thread           4,096
    # TPCs                                                        54
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.26
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block            9
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp            9
    Theoretical Occupancy                     %        14.06
    Achieved Occupancy                        %         3.71
    Achieved Active Warps Per SM           warp         2.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 73.62%                                                                                    
          The difference between calculated theoretical (14.1%) and measured achieved occupancy (3.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 85.94%                                                                                    
          The 2.25 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (14.1%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- -------------
    Metric Name                Metric Unit  Metric Value
    -------------------------- ----------- -------------
    Average DRAM Active Cycles       cycle      2,878.90
    Total DRAM Elapsed Cycles        cycle   308,474,880
    Average L1 Active Cycles         cycle  4,640,558.82
    Total L1 Elapsed Cycles          cycle   523,655,126
    Average L2 Active Cycles         cycle  2,476,691.49
    Total L2 Elapsed Cycles          cycle   365,599,920
    Average SM Active Cycles         cycle  4,640,558.82
    Total SM Elapsed Cycles          cycle   523,655,126
    Average SMSP Active Cycles       cycle  2,757,513.44
    Total SMSP Elapsed Cycles        cycle 2,094,620,504
    -------------------------- ----------- -------------

    OPT   Est. Speedup: 24.56%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.19% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.463%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 13.77% above the average, while the minimum instance value is 20.64% below the      
          average.                                                                                                      

