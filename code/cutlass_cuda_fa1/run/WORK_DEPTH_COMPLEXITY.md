# Work-Depth Complexity Analysis of Flash Attention

## Overview

This document provides a rigorous work-depth complexity analysis of our parallel Flash Attention implementations, comparing them with the baseline naive approach.

**Key Definitions:**
- **Work (W)**: Total number of operations performed
- **Depth (D)**: Length of the longest chain of dependent operations (critical path)
- **Parallelism (P)**: W/D (average available parallelism)
- **Speedup**: T_sequential / T_parallel

---

## 1. Problem Definition

**Input:**
- Q, K, V: [B, H, N, D] where
  - B = batch size
  - H = number of heads
  - N = sequence length
  - D = head dimension

**Output:**
- O: [B, H, N, D]

**Computation:**
```
O = softmax(Q @ K^T / ‚àöd) @ V
```

---

## 2. Baseline (Naive) Implementation

### 2.1 Work Complexity

**Stage 1: Compute S = Q @ K^T**
```
For each (batch, head) pair:
  For each row i in Q (N rows):
    For each row j in K (N rows):
      S[i,j] = dot(Q[i,:], K[j,:])  // D operations
      
Operations: B √ó H √ó N √ó N √ó D
Work: W‚ÇÅ = O(BHND¬≤)  // Note: This is N¬≤ in sequence dimension
```

**Stage 2: Scale S = S / ‚àöd**
```
For each element in S:
  S[i,j] = S[i,j] / ‚àöd

Operations: B √ó H √ó N √ó N √ó 1
Work: W‚ÇÇ = O(BHN¬≤)
```

**Stage 3: Softmax along rows**
```
For each row i:
  m[i] = max(S[i,:])                    // N operations
  S[i,:] = exp(S[i,:] - m[i])          // 2N operations
  l[i] = sum(S[i,:])                   // N operations
  P[i,:] = S[i,:] / l[i]               // N operations

Per row: N + 2N + N + N = 5N operations
Total rows: B √ó H √ó N
Work: W‚ÇÉ = O(BHN¬≤)
```

**Stage 4: Output O = P @ V**
```
For each position i:
  For each dimension d:
    O[i,d] = dot(P[i,:], V[:,d])  // N operations
    
Operations: B √ó H √ó N √ó D √ó N
Work: W‚ÇÑ = O(BHND¬≤)
```

**Total Work:**
```
W_baseline = W‚ÇÅ + W‚ÇÇ + W‚ÇÉ + W‚ÇÑ
           = O(BHN¬≤D) + O(BHN¬≤) + O(BHN¬≤) + O(BHN¬≤D)
           = O(BHN¬≤D)
```

### 2.2 Depth Complexity

**Stage 1: Q @ K^T (parallel over output elements)**
```
Depth: D (reduction over D elements)
Parallel over: B √ó H √ó N¬≤ output elements
```

**Stage 2: Scale**
```
Depth: O(1) (element-wise operation)
```

**Stage 3: Softmax (sequential reductions per row)**
```
max reduction: log(N) with parallel reduction
exp: O(1) element-wise
sum reduction: log(N) with parallel reduction
division: O(1) element-wise

Depth: O(log N)
```

**Stage 4: P @ V**
```
Depth: N (reduction, or log N with tree reduction)
Parallel over: B √ó H √ó N √ó D output elements
```

**Total Depth:**
```
D_baseline = O(D + 1 + log N + N)
           = O(N + D)  // N dominates for typical values
```

**Parallelism:**
```
P_baseline = W_baseline / D_baseline
           = O(BHN¬≤D) / O(N + D)
           = O(BHN¬≤D / N)  // assuming N >> D
           = O(BHND)
```

**Memory:**
```
M_baseline = O(BHN¬≤)  // Store full attention matrix
```

---

## 3. Flash Attention (Tiled) Implementation

### 3.1 Algorithm Overview

**Key innovation:** Tile-based computation with online softmax

**Tiling parameters:**
- T_r: Tile size in Q dimension (M direction)
- T_c: Tile size in K/V dimension (N direction)

**Number of tiles:**
- Q tiles: N_q = ‚åàN / T_r‚åâ
- K/V tiles: N_k = ‚åàN / T_c‚åâ

### 3.2 Work Complexity

**Outer loop:** Process each Q-tile (N_q iterations)
**Inner loop:** Process each K/V-tile (N_k iterations per Q-tile)

**Per tile iteration:**

**Step 1: Load tiles from HBM to SRAM**
```
Load Q[T_r, D]:     T_r √ó D operations
Load K[T_c, D]:     T_c √ó D operations
Load V[T_c, D]:     T_c √ó D operations

Work: O(T_r D + 2T_c D) = O((T_r + 2T_c)D)
```

**Step 2: Compute S_tile = Q_tile @ K_tile^T**
```
Operations: T_r √ó T_c √ó D √ó 2  // (multiply + add)

Work: O(T_r T_c D)
```

**Step 3: Online softmax update**
```
Compute m_new[i] = max(m_old[i], max(S_tile[i,:]))  // T_c comparisons per row
Update exp_sum via correction                        // T_c operations per row
Update O_accum                                       // T_c operations per row

Per row: 3T_c operations
Total rows: T_r
Work: O(T_r T_c)
```

**Step 4: Compute P_tile @ V_tile ‚Üí accumulate to O**
```
Operations: T_r √ó D √ó T_c √ó 2  // (multiply + add)

Work: O(T_r T_c D)
```

**Work per tile iteration:**
```
W_tile = O(T_r D) + O(T_c D) + O(T_r T_c D) + O(T_r T_c) + O(T_r T_c D)
       = O(T_r T_c D)  // Dominant term
```

**Total work per Q-tile:**
```
W_q_tile = N_k √ó W_tile
         = (N / T_c) √ó O(T_r T_c D)
         = O(T_r N D)
```

**Total work for all Q-tiles:**
```
W_flash = N_q √ó W_q_tile
        = (N / T_r) √ó O(T_r N D)
        = O(N¬≤ D)

Per (batch, head): O(N¬≤ D)
Total: W_flash = O(BHN¬≤D)
```

**Conclusion:** Same asymptotic work as baseline! ‚úÖ

### 3.3 Depth Complexity

**Parallelization strategy:**
- Grid dimension: (N_q, H, B) for Small Tile
- Each block processes one Q-tile independently
- Within block: threads cooperate on tile computations

**Critical path analysis:**

**Per Q-tile processing (sequential over K/V tiles):**
```
For k_tile = 0 to N_k-1:  // Sequential loop (cannot parallelize due to online softmax dependencies)
    Load tiles:           O(1) with parallel threads
    Compute S_tile:       O(D) depth (reduction over D)
    Update softmax:       O(log T_c) with parallel reduction
    Compute P@V:          O(T_c) or O(log T_c) with tree reduction
    Accumulate to O:      O(1)
```

**Depth per K-tile iteration:**
```
D_k_iter = O(1) + O(D) + O(log T_c) + O(log T_c) + O(1)
         = O(D + log T_c)
         = O(D)  // Assuming D >> log T_c
```

**Total depth per Q-tile:**
```
D_q_tile = N_k √ó D_k_iter
         = (N / T_c) √ó O(D)
         = O(ND / T_c)
```

**Parallel over Q-tiles:**
```
All Q-tiles process in parallel (different blocks)
Depth = depth of one Q-tile
```

**Total depth:**
```
D_flash = O(ND / T_c)
```

**With typical values (Small Tile):**
- N = 1024
- D = 32
- T_c = 90

```
D_flash = O(1024 √ó 32 / 90) = O(364)
```

### 3.4 Parallelism Analysis

```
P_flash = W_flash / D_flash
        = O(BHN¬≤D) / O(ND / T_c)
        = O(BHN¬≤D √ó T_c / ND)
        = O(BHNT_c)
```

**Example (Small Tile: B=1, H=16, N=1024, T_c=90):**
```
P_flash = 1 √ó 16 √ó 1024 √ó 90
        = 1,474,560 operations can run in parallel
```

**Comparison with baseline:**
```
P_baseline = O(BHND) = 1 √ó 16 √ó 1024 √ó 32 = 524,288
P_flash = O(BHNT_c) = 1 √ó 16 √ó 1024 √ó 90 = 1,474,560

P_flash / P_baseline = T_c / D = 90 / 32 = 2.8x more parallelism! ‚úÖ
```

### 3.5 Memory Complexity

**Per block:**
```
Q_tile: T_r √ó D
K_tile: T_c √ó D
V_tile: T_c √ó D
S_tile: T_r √ó T_c
P_tile: T_r √ó T_c
m, l arrays: 2 √ó T_r
O_accum: T_r √ó D

M_block = O(T_r D + 2T_c D + 2T_r T_c + T_r)
        = O(T_r T_c + (T_r + T_c)D)  // For typical cases where T_r, T_c > D
```

**Global memory:**
```
M_global = O(BHND)  // Only store Q, K, V, O
         << O(BHN¬≤)  // Baseline stores attention matrix!
```

**Memory reduction:**
```
M_baseline / M_flash = O(BHN¬≤) / O(BHND)
                     = O(N / D)
                     = 1024 / 32 = 32x reduction! üéâ
```

---

## 4. Detailed Comparison

### 4.1 Complexity Table

| Metric | Baseline | Flash Attention | Improvement |
|--------|----------|----------------|-------------|
| **Work** | O(BHN¬≤D) | O(BHN¬≤D) | Same |
| **Depth** | O(N + D) | O(ND/T_c) | Worse by factor of D/T_c |
| **Parallelism** | O(BHND) | O(BHNT_c) | Better by factor of T_c/D |
| **Memory** | O(BHN¬≤) | O(BHND) | Better by factor of N/D |
| **Memory/block** | N/A | O(T_rT_c + (T_r+T_c)D) | Constant per block |

### 4.2 Why Flash Attention is Faster Despite Higher Depth?

**Key insight:** Flash Attention is **memory-bound**, not **compute-bound**!

**Baseline bottleneck:**
```
HBM Access: O(BHN¬≤) memory movements
            = 1 √ó 16 √ó 1024¬≤ √ó 4 bytes (FP32)
            = 67.1 MB for attention matrix alone
            
HBM bandwidth: 1555 GB/s (A100)
Time: 67.1 MB / 1555 GB/s = 0.043 ms
```

**Flash Attention:**
```
HBM Access: O(BHND) only (no attention matrix)
            = 1 √ó 16 √ó 1024 √ó 32 √ó 2 bytes (FP16)
            = 1.05 MB for Q/K/V/O
            
Time: 1.05 MB / 1555 GB/s = 0.0007 ms
```

**Memory access reduction:**
```
67.1 MB / 1.05 MB ‚âà 64x less data movement! üöÄ
```

**Roofline analysis:**

```
Arithmetic Intensity = Work / Memory
                     = N¬≤D / ND  (for attention per head)
                     = N

For N=1024: AI = 1024 ops/byte

A100 roofline: 312 TFLOPs / 1555 GB/s = 200 ops/byte

Since 1024 >> 200, we're compute-bound in theory.
But due to tile-based execution, effective AI is lower.
Flash Attention stays memory-bound but with less memory traffic.
```

### 4.3 Concrete Example (Small Tile Config)

**Configuration:**
- B = 1, H = 16, N = 1024, D = 32
- T_r = 45, T_c = 90

**Work:**
```
W_flash = B √ó H √ó N¬≤ √ó D √ó 2
        = 1 √ó 16 √ó 1024¬≤ √ó 32 √ó 2
        = 1,099,511,627,776 operations
        ‚âà 1.1 trillion operations
```

**Depth:**
```
D_flash = N √ó D / T_c
        = 1024 √ó 32 / 90
        ‚âà 364 operations (critical path length)
```

**Parallelism:**
```
P_flash = W_flash / D_flash
        = 1.1T / 364
        ‚âà 3 billion operations can run in parallel
```

**Actual GPU parallelism:**
```
Grid size: (23, 16, 1) = 368 blocks
Threads per block: 256
Total threads: 368 √ó 256 = 94,208 threads

Utilization: 94,208 / 3B = 0.003% of available parallelism üò±
```

**This explains why we only achieve 1.02 TFLOPs/s (0.3% of A100's 312 TFLOPs/s peak)!**

---

## 5. Impact of Tile Sizes

### 5.1 Tile Size Trade-offs

**Larger tiles (T_r, T_c ‚Üë):**

‚úÖ **Pros:**
- Lower depth: D = O(ND/T_c) ‚Üì
- Fewer kernel launches: N_q √ó N_k ‚Üì
- Better data reuse

‚ùå **Cons:**
- More shared memory: M_block = O(T_rT_c) ‚Üë
- Lower occupancy: blocks/SM ‚Üì
- Less grid-level parallelism

**Smaller tiles:**

‚úÖ **Pros:**
- Less shared memory ‚Üí higher occupancy
- More grid-level parallelism: more blocks

‚ùå **Cons:**
- Higher depth: more iterations
- More kernel launch overhead
- Less data reuse per tile

### 5.2 Optimal Tile Size Analysis

**Constraint:** Shared memory limit
```
M_block ‚â§ M_SM / blocks_per_SM
51.7 KB ‚â§ 164 KB / 3  (for Small Tile)
```

**Sweet spot:** Balance between:
1. Maximize T_r √ó T_c (minimize depth)
2. Minimize M_block (maximize occupancy)
3. Ensure enough grid parallelism (N_q √ó H √ó B ‚â´ num_SMs)

**Small Tile choice (45√ó90):**
```
Shared memory: 51.7 KB ‚Üí 3 blocks/SM ‚Üí 37.5% occupancy ‚úÖ
Grid size: 23 √ó 16 = 368 blocks >> 108 SMs ‚úÖ
Depth: 1024 √ó 32 / 90 ‚âà 364 iterations (acceptable)
```

**Large Tile choice (120√ó120):**
```
Shared memory: 150.9 KB ‚Üí 1 block/SM ‚Üí 3.1% occupancy ‚ùå
Grid size: 9 √ó 16 = 144 blocks > 108 SMs (marginal)
Depth: 1024 √ó 32 / 120 ‚âà 273 iterations (better, but occupancy kills it)
```

---

## 6. Work-Depth Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    BASELINE ATTENTION                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                              ‚îÇ
‚îÇ  Work: W = BHN¬≤D                                            ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇQ @ K^T ‚îÇ ‚Üí ‚îÇ Scale  ‚îÇ ‚Üí ‚îÇSoftmax ‚îÇ ‚Üí ‚îÇ P @ V  ‚îÇ        ‚îÇ
‚îÇ  ‚îÇO(N¬≤D)  ‚îÇ   ‚îÇO(N¬≤)   ‚îÇ   ‚îÇO(N¬≤)   ‚îÇ   ‚îÇO(N¬≤D)  ‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ      ‚Üì             ‚Üì            ‚Üì            ‚Üì              ‚îÇ
‚îÇ   Depth:O(D)    O(1)       O(logN)      O(N)               ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  Critical Path: D_baseline = O(N + D)                      ‚îÇ
‚îÇ  Parallelism: P = O(BHND)                                  ‚îÇ
‚îÇ  Memory: O(BHN¬≤) ‚Üê QUADRATIC! üò±                           ‚îÇ
‚îÇ                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  FLASH ATTENTION (TILED)                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                              ‚îÇ
‚îÇ  Work: W = BHN¬≤D (SAME as baseline!)                       ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  Grid: (N/T_r) √ó H √ó B blocks (parallel)                   ‚îÇ
‚îÇ  ‚îÇ                                                           ‚îÇ
‚îÇ  ‚îî‚îÄ‚ñ∫ Per Q-tile block:                                      ‚îÇ
‚îÇ       ‚îÇ                                                      ‚îÇ
‚îÇ       ‚îî‚îÄ‚ñ∫ For k=0 to N/T_c-1: (SEQUENTIAL!)                ‚îÇ
‚îÇ            ‚îÇ                                                 ‚îÇ
‚îÇ            ‚îú‚îÄ‚ñ∫ Load Q_tile, K_tile, V_tile  : O(1)         ‚îÇ
‚îÇ            ‚îú‚îÄ‚ñ∫ Compute S = Q @ K^T          : O(D)         ‚îÇ
‚îÇ            ‚îú‚îÄ‚ñ∫ Update online softmax        : O(logT_c)    ‚îÇ
‚îÇ            ‚îú‚îÄ‚ñ∫ Compute P @ V                : O(logT_c)    ‚îÇ
‚îÇ            ‚îî‚îÄ‚ñ∫ Accumulate to O              : O(1)         ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ            Depth per iteration: O(D)                        ‚îÇ
‚îÇ            Total iterations: N/T_c                          ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  Critical Path: D_flash = O(ND/T_c)                        ‚îÇ
‚îÇ  Parallelism: P = O(BHNT_c)                                ‚îÇ
‚îÇ  Memory: O(BHND) ‚Üê LINEAR! üéâ                              ‚îÇ
‚îÇ                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Parallelism Comparison:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Baseline:        P_base = O(BHND)
                          ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
                          ‚ïë   Available    ‚ïë
                          ‚ïë  Parallelism   ‚ïë
                          ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Flash Attention: P_flash = O(BHNT_c)
                          ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
                          ‚ïë    Available Parallelism   ‚ïë
                          ‚ïë         (T_c/D √ó more)     ‚ïë
                          ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Where T_c/D ‚âà 90/32 = 2.8x for Small Tile
```

---

## 7. Summary

### 7.1 Key Results

| Metric | Baseline | Flash Attention | Winner |
|--------|----------|----------------|--------|
| **Work** | O(BHN¬≤D) | O(BHN¬≤D) | Tie |
| **Depth** | O(N) | O(ND/T_c) | Baseline (lower depth) |
| **Parallelism** | O(BHND) | O(BHNT_c) | Flash (2.8x more) |
| **Memory** | O(BHN¬≤) | O(BHND) | Flash (32x less) |
| **Actual Perf** | 0.27 TFLOPs/s | 1.02 TFLOPs/s | Flash (3.8x faster) |

### 7.2 Why Flash Attention Wins

Despite having **worse depth complexity**, Flash Attention is faster because:

1. **Memory bottleneck dominates:** Reducing O(N¬≤) ‚Üí O(N) memory is crucial
2. **Higher parallelism available:** More work can happen simultaneously
3. **Better cache behavior:** Tiled execution exploits memory hierarchy
4. **Reduced HBM traffic:** 64x less data movement

### 7.3 Theoretical vs Actual Performance Gap

**Theoretical parallelism:** P = 3 billion operations
**Actual GPU threads:** 94,208 threads
**Utilization:** 0.003%

**Reasons for gap:**
1. Problem size too small (N=1024, B=1)
2. Memory-bound workload (not compute-bound)
3. Sequential K/V iteration (depth bottleneck)
4. Insufficient grid parallelism

**To improve:**
- Increase batch size (B ‚Üë)
- Increase sequence length (N ‚Üë)
- Multi-GPU scaling
- Further optimization of memory access patterns

---

## 8. Conclusion

Flash Attention achieves:
- ‚úÖ **Same work complexity** O(BHN¬≤D)
- ‚ö†Ô∏è **Higher depth** O(ND/T_c) vs O(N)
- ‚úÖ **More parallelism** O(BHNT_c) vs O(BHND)
- ‚úÖ **Linear memory** O(N) vs O(N¬≤)
- üöÄ **3.8x faster** in practice

**The key insight:** Memory complexity reduction outweighs depth complexity increase for attention computation on modern GPUs!

---

**End of Analysis**

