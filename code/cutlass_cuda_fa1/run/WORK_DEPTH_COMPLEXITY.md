# Work-Depth Complexity Analysis of Flash Attention

## Overview

This document provides a rigorous work-depth complexity analysis of our parallel Flash Attention implementations, comparing them with the baseline naive approach.

**Key Definitions:**
- **Work (W)**: Total number of operations performed
- **Depth (D)**: Length of the longest chain of dependent operations (critical path)
- **Parallelism (P)**: W/D (average available parallelism)
- **Speedup**: T_sequential / T_parallel

---

## 1. Problem Definition

**Input:**
- Q, K, V: [B, H, N, D] where
  - B = batch size
  - H = number of heads
  - N = sequence length
  - D = head dimension

**Output:**
- O: [B, H, N, D]

**Computation:**
```
O = softmax(Q @ K^T / âˆšd) @ V
```

---

## 2. Baseline (Naive) Implementation

### 2.1 Work Complexity

**Stage 1: Compute S = Q @ K^T**
```
For each (batch, head) pair:
  For each row i in Q (N rows):
    For each row j in K (N rows):
      S[i,j] = dot(Q[i,:], K[j,:])  // D operations
      
Operations: B Ã— H Ã— N Ã— N Ã— D
Work: Wâ‚ = O(BHNDÂ²)  // Note: This is NÂ² in sequence dimension
```

**Stage 2: Scale S = S / âˆšd**
```
For each element in S:
  S[i,j] = S[i,j] / âˆšd

Operations: B Ã— H Ã— N Ã— N Ã— 1
Work: Wâ‚‚ = O(BHNÂ²)
```

**Stage 3: Softmax along rows**
```
For each row i:
  m[i] = max(S[i,:])                    // N operations
  S[i,:] = exp(S[i,:] - m[i])          // 2N operations
  l[i] = sum(S[i,:])                   // N operations
  P[i,:] = S[i,:] / l[i]               // N operations

Per row: N + 2N + N + N = 5N operations
Total rows: B Ã— H Ã— N
Work: Wâ‚ƒ = O(BHNÂ²)
```

**Stage 4: Output O = P @ V**
```
For each position i:
  For each dimension d:
    O[i,d] = dot(P[i,:], V[:,d])  // N operations
    
Operations: B Ã— H Ã— N Ã— D Ã— N
Work: Wâ‚„ = O(BHNDÂ²)
```

**Total Work:**
```
W_baseline = Wâ‚ + Wâ‚‚ + Wâ‚ƒ + Wâ‚„
           = O(BHNÂ²D) + O(BHNÂ²) + O(BHNÂ²) + O(BHNÂ²D)
           = O(BHNÂ²D)
```

### 2.2 Depth Complexity

**Stage 1: Q @ K^T (parallel over output elements)**
```
Depth: D (reduction over D elements)
Parallel over: B Ã— H Ã— NÂ² output elements
```

**Stage 2: Scale**
```
Depth: O(1) (element-wise operation)
```

**Stage 3: Softmax (sequential reductions per row)**
```
max reduction: log(N) with parallel reduction
exp: O(1) element-wise
sum reduction: log(N) with parallel reduction
division: O(1) element-wise

Depth: O(log N)
```

**Stage 4: P @ V**
```
Depth: N (reduction, or log N with tree reduction)
Parallel over: B Ã— H Ã— N Ã— D output elements
```

**Total Depth:**
```
D_baseline = O(D + 1 + log N + N)
           = O(N + D)  // N dominates for typical values
```

**Parallelism:**
```
P_baseline = W_baseline / D_baseline
           = O(BHNÂ²D) / O(N + D)
           = O(BHNÂ²D / N)  // assuming N >> D
           = O(BHND)
```

**Memory:**
```
M_baseline = O(BHNÂ²)  // Store full attention matrix
```

---

## 3. Flash Attention (Tiled) Implementation

### 3.1 Algorithm Overview

**Key innovation:** Tile-based computation with online softmax

**Tiling parameters:**
- T_r: Tile size in Q dimension (M direction)
- T_c: Tile size in K/V dimension (N direction)

**Number of tiles:**
- Q tiles: N_q = âŒˆN / T_râŒ‰
- K/V tiles: N_k = âŒˆN / T_câŒ‰

### 3.2 Work Complexity

**Outer loop:** Process each Q-tile (N_q iterations)
**Inner loop:** Process each K/V-tile (N_k iterations per Q-tile)

**Per tile iteration:**

**Step 1: Load tiles from HBM to SRAM**
```
Load Q[T_r, D]:     T_r Ã— D operations
Load K[T_c, D]:     T_c Ã— D operations
Load V[T_c, D]:     T_c Ã— D operations

Work: O(T_r D + 2T_c D) = O((T_r + 2T_c)D)
```

**Step 2: Compute S_tile = Q_tile @ K_tile^T**
```
Operations: T_r Ã— T_c Ã— D Ã— 2  // (multiply + add)

Work: O(T_r T_c D)
```

**Step 3: Online softmax update**
```
Compute m_new[i] = max(m_old[i], max(S_tile[i,:]))  // T_c comparisons per row
Update exp_sum via correction                        // T_c operations per row
Update O_accum                                       // T_c operations per row

Per row: 3T_c operations
Total rows: T_r
Work: O(T_r T_c)
```

**Step 4: Compute P_tile @ V_tile â†’ accumulate to O**
```
Operations: T_r Ã— D Ã— T_c Ã— 2  // (multiply + add)

Work: O(T_r T_c D)
```

**Work per tile iteration:**
```
W_tile = O(T_r D) + O(T_c D) + O(T_r T_c D) + O(T_r T_c) + O(T_r T_c D)
       = O(T_r T_c D)  // Dominant term
```

**Total work per Q-tile:**
```
W_q_tile = N_k Ã— W_tile
         = (N / T_c) Ã— O(T_r T_c D)
         = O(T_r N D)
```

**Total work for all Q-tiles:**
```
W_flash = N_q Ã— W_q_tile
        = (N / T_r) Ã— O(T_r N D)
        = O(NÂ² D)

Per (batch, head): O(NÂ² D)
Total: W_flash = O(BHNÂ²D)
```

**Conclusion:** Same asymptotic work as baseline! âœ…

### 3.3 Depth Complexity

**Parallelization strategy:**
- Grid dimension: (N_q, H, B) for Small Tile
- Each block processes one Q-tile independently
- Within block: threads cooperate on tile computations

**Critical path analysis:**

**Per Q-tile processing (sequential over K/V tiles):**
```
For k_tile = 0 to N_k-1:  // Sequential loop (cannot parallelize due to online softmax dependencies)
    Load tiles:           O(1) with parallel threads
    Compute S_tile:       O(D) depth (reduction over D)
    Update softmax:       O(log T_c) with parallel reduction
    Compute P@V:          O(T_c) or O(log T_c) with tree reduction
    Accumulate to O:      O(1)
```

**Depth per K-tile iteration:**
```
D_k_iter = O(1) + O(D) + O(log T_c) + O(log T_c) + O(1)
         = O(D + log T_c)
         = O(D)  // Assuming D >> log T_c
```

**Total depth per Q-tile:**
```
D_q_tile = N_k Ã— D_k_iter
         = (N / T_c) Ã— O(D)
         = O(ND / T_c)
```

**Parallel over Q-tiles:**
```
All Q-tiles process in parallel (different blocks)
Depth = depth of one Q-tile
```

**Total depth:**
```
D_flash = O(ND / T_c)
```

**With typical values (Small Tile):**
- N = 1024
- D = 32
- T_c = 90

```
D_flash = O(1024 Ã— 32 / 90) = O(364)
```

### 3.4 Parallelism Analysis

```
P_flash = W_flash / D_flash
        = O(BHNÂ²D) / O(ND / T_c)
        = O(BHNÂ²D Ã— T_c / ND)
        = O(BHNT_c)
```

**Example (Small Tile: B=1, H=16, N=1024, T_c=90):**
```
P_flash = 1 Ã— 16 Ã— 1024 Ã— 90
        = 1,474,560 operations can run in parallel
```

**Comparison with baseline:**
```
P_baseline = O(BHND) = 1 Ã— 16 Ã— 1024 Ã— 32 = 524,288
P_flash = O(BHNT_c) = 1 Ã— 16 Ã— 1024 Ã— 90 = 1,474,560

P_flash / P_baseline = T_c / D = 90 / 32 = 2.8x more parallelism! âœ…
```

### 3.5 Memory Complexity

**Per block:**
```
Q_tile: T_r Ã— D
K_tile: T_c Ã— D
V_tile: T_c Ã— D
S_tile: T_r Ã— T_c
P_tile: T_r Ã— T_c
m, l arrays: 2 Ã— T_r
O_accum: T_r Ã— D

M_block = O(T_r D + 2T_c D + 2T_r T_c + T_r)
        = O(T_r T_c + (T_r + T_c)D)  // For typical cases where T_r, T_c > D
```

**Global memory:**
```
M_global = O(BHND)  // Only store Q, K, V, O
         << O(BHNÂ²)  // Baseline stores attention matrix!
```

**Memory reduction:**
```
M_baseline / M_flash = O(BHNÂ²) / O(BHND)
                     = O(N / D)
                     = 1024 / 32 = 32x reduction! ğŸ‰
```

---

## 4. Detailed Comparison

### 4.1 Complexity Table

| Metric | Baseline | Flash Attention | Improvement |
|--------|----------|----------------|-------------|
| **Work** | O(BHNÂ²D) | O(BHNÂ²D) | Same |
| **Depth** | O(N + D) | O(ND/T_c) | Worse by factor of D/T_c |
| **Parallelism** | O(BHND) | O(BHNT_c) | Better by factor of T_c/D |
| **Memory** | O(BHNÂ²) | O(BHND) | Better by factor of N/D |
| **Memory/block** | N/A | O(T_rT_c + (T_r+T_c)D) | Constant per block |

### 4.2 Why Flash Attention is Faster Despite Higher Depth?

**Key insight:** Flash Attention is **memory-bound**, not **compute-bound**!

**Baseline bottleneck:**
```
HBM Access: O(BHNÂ²) memory movements
            = 1 Ã— 16 Ã— 1024Â² Ã— 4 bytes (FP32)
            = 67.1 MB for attention matrix alone
            
HBM bandwidth: 1555 GB/s (A100)
Time: 67.1 MB / 1555 GB/s = 0.043 ms
```

**Flash Attention:**
```
HBM Access: O(BHND) only (no attention matrix)
            = 1 Ã— 16 Ã— 1024 Ã— 32 Ã— 2 bytes (FP16)
            = 1.05 MB for Q/K/V/O
            
Time: 1.05 MB / 1555 GB/s = 0.0007 ms
```

**Memory access reduction:**
```
67.1 MB / 1.05 MB â‰ˆ 64x less data movement! ğŸš€
```

**Roofline analysis:**

```
Arithmetic Intensity = Work / Memory
                     = NÂ²D / ND  (for attention per head)
                     = N

For N=1024: AI = 1024 ops/byte

A100 roofline: 312 TFLOPs / 1555 GB/s = 200 ops/byte

Since 1024 >> 200, we're compute-bound in theory.
But due to tile-based execution, effective AI is lower.
Flash Attention stays memory-bound but with less memory traffic.
```

### 4.3 Concrete Example (Small Tile Config)

**Configuration:**
- B = 1, H = 16, N = 1024, D = 32
- T_r = 45, T_c = 90

**Work:**
```
W_flash = B Ã— H Ã— NÂ² Ã— D Ã— 2
        = 1 Ã— 16 Ã— 1024Â² Ã— 32 Ã— 2
        = 1,099,511,627,776 operations
        â‰ˆ 1.1 trillion operations
```

**Depth:**
```
D_flash = N Ã— D / T_c
        = 1024 Ã— 32 / 90
        â‰ˆ 364 operations (critical path length)
```

**Parallelism:**
```
P_flash = W_flash / D_flash
        = 1.1T / 364
        â‰ˆ 3 billion operations can run in parallel
```

**Actual GPU parallelism:**
```
Grid size: (23, 16, 1) = 368 blocks
Threads per block: 256
Total threads: 368 Ã— 256 = 94,208 threads

Utilization: 94,208 / 3B = 0.003% of available parallelism ğŸ˜±
```

**This explains why we only achieve 1.02 TFLOPs/s (0.3% of A100's 312 TFLOPs/s peak)!**

---

## 5. Impact of Tile Sizes

### 5.1 Tile Size Trade-offs

**Larger tiles (T_r, T_c â†‘):**

âœ… **Pros:**
- Lower depth: D = O(ND/T_c) â†“
- Fewer kernel launches: N_q Ã— N_k â†“
- Better data reuse

âŒ **Cons:**
- More shared memory: M_block = O(T_rT_c) â†‘
- Lower occupancy: blocks/SM â†“
- Less grid-level parallelism

**Smaller tiles:**

âœ… **Pros:**
- Less shared memory â†’ higher occupancy
- More grid-level parallelism: more blocks

âŒ **Cons:**
- Higher depth: more iterations
- More kernel launch overhead
- Less data reuse per tile

### 5.2 Optimal Tile Size Analysis

**Constraint:** Shared memory limit
```
M_block â‰¤ M_SM / blocks_per_SM
51.7 KB â‰¤ 164 KB / 3  (for Small Tile)
```

**Sweet spot:** Balance between:
1. Maximize T_r Ã— T_c (minimize depth)
2. Minimize M_block (maximize occupancy)
3. Ensure enough grid parallelism (N_q Ã— H Ã— B â‰« num_SMs)

**Small Tile choice (45Ã—90):**
```
Shared memory: 51.7 KB â†’ 3 blocks/SM â†’ 37.5% occupancy âœ…
Grid size: 23 Ã— 16 = 368 blocks >> 108 SMs âœ…
Depth: 1024 Ã— 32 / 90 â‰ˆ 364 iterations (acceptable)
```

**Large Tile choice (120Ã—120):**
```
Shared memory: 150.9 KB â†’ 1 block/SM â†’ 3.1% occupancy âŒ
Grid size: 9 Ã— 16 = 144 blocks > 108 SMs (marginal)
Depth: 1024 Ã— 32 / 120 â‰ˆ 273 iterations (better, but occupancy kills it)
```

---

## 6. Work-Depth Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BASELINE ATTENTION                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Work: W = BHNÂ²D                                            â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚Q @ K^T â”‚ â†’ â”‚ Scale  â”‚ â†’ â”‚Softmax â”‚ â†’ â”‚ P @ V  â”‚        â”‚
â”‚  â”‚O(NÂ²D)  â”‚   â”‚O(NÂ²)   â”‚   â”‚O(NÂ²)   â”‚   â”‚O(NÂ²D)  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚      â†“             â†“            â†“            â†“              â”‚
â”‚   Depth:O(D)    O(1)       O(logN)      O(N)               â”‚
â”‚                                                              â”‚
â”‚  Critical Path: D_baseline = O(N + D)                      â”‚
â”‚  Parallelism: P = O(BHND)                                  â”‚
â”‚  Memory: O(BHNÂ²) â† QUADRATIC! ğŸ˜±                           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  FLASH ATTENTION (TILED)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Work: W = BHNÂ²D (SAME as baseline!)                       â”‚
â”‚                                                              â”‚
â”‚  Grid: (N/T_r) Ã— H Ã— B blocks (parallel)                   â”‚
â”‚  â”‚                                                           â”‚
â”‚  â””â”€â–º Per Q-tile block:                                      â”‚
â”‚       â”‚                                                      â”‚
â”‚       â””â”€â–º For k=0 to N/T_c-1: (SEQUENTIAL!)                â”‚
â”‚            â”‚                                                 â”‚
â”‚            â”œâ”€â–º Load Q_tile, K_tile, V_tile  : O(1)         â”‚
â”‚            â”œâ”€â–º Compute S = Q @ K^T          : O(D)         â”‚
â”‚            â”œâ”€â–º Update online softmax        : O(logT_c)    â”‚
â”‚            â”œâ”€â–º Compute P @ V                : O(logT_c)    â”‚
â”‚            â””â”€â–º Accumulate to O              : O(1)         â”‚
â”‚                                                              â”‚
â”‚            Depth per iteration: O(D)                        â”‚
â”‚            Total iterations: N/T_c                          â”‚
â”‚                                                              â”‚
â”‚  Critical Path: D_flash = O(ND/T_c)                        â”‚
â”‚  Parallelism: P = O(BHNT_c)                                â”‚
â”‚  Memory: O(BHND) â† LINEAR! ğŸ‰                              â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Parallelism Comparison:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Baseline:        P_base = O(BHND)
                          â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
                          â•‘   Available    â•‘
                          â•‘  Parallelism   â•‘
                          â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Flash Attention: P_flash = O(BHNT_c)
                          â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
                          â•‘    Available Parallelism   â•‘
                          â•‘         (T_c/D Ã— more)     â•‘
                          â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Where T_c/D â‰ˆ 90/32 = 2.8x for Small Tile
```

---

## 7. Summary

### 7.1 Key Results

| Metric | Baseline | Flash Attention | Winner |
|--------|----------|----------------|--------|
| **Work** | O(BHNÂ²D) | O(BHNÂ²D) | Tie |
| **Depth** | O(N) | O(ND/T_c) | Baseline (lower depth) |
| **Parallelism** | O(BHND) | O(BHNT_c) | Flash (2.8x more) |
| **Memory** | O(BHNÂ²) | O(BHND) | Flash (32x less) |
| **Actual Perf** | 0.27 TFLOPs/s | 1.02 TFLOPs/s | Flash (3.8x faster) |

### 7.2 Why Flash Attention Wins

Despite having **worse depth complexity**, Flash Attention is faster because:

1. **Memory bottleneck dominates:** Reducing O(NÂ²) â†’ O(N) memory is crucial
2. **Higher parallelism available:** More work can happen simultaneously
3. **Better cache behavior:** Tiled execution exploits memory hierarchy
4. **Reduced HBM traffic:** 64x less data movement

### 7.3 Theoretical vs Actual Performance Gap

**Theoretical parallelism:** P = 3 billion operations
**Actual GPU threads:** 94,208 threads
**Utilization:** 0.003%

**Reasons for gap:**
1. Problem size too small (N=1024, B=1)
2. Memory-bound workload (not compute-bound)
3. Sequential K/V iteration (depth bottleneck)
4. Insufficient grid parallelism

**To improve:**
- Increase batch size (B â†‘)
- Increase sequence length (N â†‘)
- Multi-GPU scaling
- Further optimization of memory access patterns

---

## 8. Conclusion

Flash Attention achieves:
- âœ… **Same work complexity** O(BHNÂ²D)
- âš ï¸ **Higher depth** O(ND/T_c) vs O(N)
- âœ… **More parallelism** O(BHNT_c) vs O(BHND)
- âœ… **Linear memory** O(N) vs O(NÂ²)
- ğŸš€ **3.8x faster** in practice

**The key insight:** Memory complexity reduction outweighs depth complexity increase for attention computation on modern GPUs!

---

**End of Analysis**

