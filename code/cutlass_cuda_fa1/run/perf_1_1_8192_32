==========================================
Minimal Flash Attention - Quick Start
==========================================

[0;32m‚úì[0m CUDA found: 12.8
[0;32m‚úì[0m GPU detected: NVIDIA A100-SXM4-40GB
  Using CUDA architecture: -arch=sm_80
[0;32m‚úì[0m Cutlass found at ../csrc/cutlass

==========================================
Building...
==========================================
Using Makefile build system...
rm -f test_flash_attn *.o
Compiling test_flash_attn...
CUDA Architecture: -arch=sm_80
Cutlass Include: ../csrc/cutlass/include
nvcc -arch=sm_80 -std=c++17 -O3 --expt-relaxed-constexpr --expt-extended-lambda -I../csrc/cutlass/include -I. --use_fast_math -o test_flash_attn test_flash_attn.cu flash_attn_unified.cu flash_attn_cutlass.cu
Build complete: ./test_flash_attn
[0;32m‚úì[0m Build successful!

==========================================
Running tests...
==========================================

Flash Attention Minimal Implementation Test
================================================================================
GPU: NVIDIA A100-SXM4-40GB
Compute Capability: 8.0
================================================================================

================================================================================
Flash Attention Performance Test: Tile Size & CUTLASS Comparison
================================================================================

All Flash Attention variants use the same algorithm (online softmax + tiling)
Differences are in tile size and compute primitives:

  Small Tile:    45√ó90 tiles, 128 threads, 51.7 KB shared mem
    ‚Üí Conservative config, standard CUDA cores

  CUTLASS TC:    45√ó90 tiles, 128 threads, 51.7 KB shared mem + Tensor Cores
    ‚Üí Same config as Small Tile, but uses A100 tensor cores for GEMMs

  Large Tile:    120√ó120 tiles, 256 threads, 150.9 KB shared mem
    ‚Üí Aggressive config, standard CUDA cores

  Baseline:      O(batch √ó heads √ó seq_len¬≤) memory ‚Üê QUADRATIC!


================================================================================
Config: batch=1, heads=1, seqlen=8192, headdim=32
================================================================================
Memory per tensor: 0.50 MB (Q/K/V/O)
Baseline scores buffer: 256.00 MB (batch√óheads√óseq¬≤√ó4bytes)
Total memory: 258.00 MB (+ 1 MB for CUTLASS output)
Initializing inputs...
Running Baseline (Naive, no shared mem, no online softmax)...
Running Flash Attention (Small Tile: conservative config)...
Running Flash Attention (Large Tile: aggressive config)...

================================================================================
Flash Attention - Large Tile Configuration (head_dim=32)
================================================================================
  Tile Config for head_dim=32:
    Tile size: 120x120
    Threads: 64
    Shared memory: 150.9 KB
================================================================================
Running Flash Attention (CUTLASS Tensor Core: same tile as Small)...

================================================================================
Flash Attention - WMMA Tensor Core (head_dim=32)
================================================================================
  Tile size: 45x90 (same as Small Tile)
  Threads: 256 (8 warps)
  Shared memory: 51.7 KB
  Tensor Cores: ENABLED via WMMA API
    ‚Üí Q@K^T: wmma::mma_sync (16x16x16 tiles, FP16‚ÜíFP32)
    ‚Üí P@V:   CUDA cores (FP32 input limitation)
================================================================================

Verifying correctness...
Comparing Large Tile vs Small Tile:

Comparing CUTLASS vs Small Tile:

Comparing Baseline vs Small Tile:

Comparing Large Tile vs Baseline:

================================================================================
Performance Results:
================================================================================
Baseline (Naive):                       95.360 ms  (1.00x vs baseline)
Flash Attn (Small Tile):                13.383 ms  (7.13x vs baseline)
Flash Attn (CUTLASS Tensor Core):       13.021 ms  (7.32x vs baseline, 1.03x vs Small)
Flash Attn (Large Tile):                34.440 ms  (2.77x vs baseline)

================================================================================
Accuracy Results (symmetric relative error):
================================================================================
Large Tile vs Small Tile:  0.000574
CUTLASS vs Small Tile:     0.000375
Baseline vs Small Tile:    0.000581
Large Tile vs Baseline:    0.000581

‚úÖ TEST PASSED (All implementations agree within 2.0%)

================================================================================
Throughput:
================================================================================
Total FLOPs:                   8.59 GFLOPs (8589.93 million ops)
Baseline (Naive):              0.09 TFLOPs/s
Flash Attn (Small Tile):       0.64 TFLOPs/s (7.13x vs baseline)
Flash Attn (CUTLASS TC):       0.66 TFLOPs/s (7.32x vs baseline, 1.03x vs Small)
Flash Attn (Large Tile):       0.25 TFLOPs/s (2.77x vs baseline)

Memory Bandwidth:
Flash Attn (CUTLASS TC):       0.16 GB/s
Flash Attn (Large Tile):       0.06 GB/s
A100 HBM peak:                 ~1555 GB/s

Note: Attention is memory-bound. CUTLASS tensor cores help but limited by bandwidth.
      To see higher TFLOPs, use larger batch sizes or longer sequences.

================================================================================
All tests completed!

==========================================
Done!
==========================================

To rebuild:
  build_and_run.sh

To use CMake:
  build_and_run.sh cmake

To clean:
  make clean
  rm -rf build/
