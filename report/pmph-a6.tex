\pdfoutput=1

\documentclass[11pt]{article}

\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}
\usepackage{hyperref}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{graphicx}

\usepackage{natbib}

\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{etoolbox}
\usepackage[table]{xcolor}
\usepackage[margin=1in]{geometry}

% Define commands for highlighting the minimum value in a row
\newcolumntype{R}{S[table-format=4.3]}
\newrobustcmd{\B}[1]{%
  \ifdimless{#1pt}{9999pt}{\bfseries\color{blue!75!black}}{}%
  #1
}
\newcommand{\mcc}[1]{\multicolumn{1}{c}{#1}}

% for citation commands in the .tex, authors can use:
% \citep, \citet, and \citeyearpar for compatibility with natbib, or
% \cite, \newcite, and \shortcite for compatibility with older ACL .sty files
\renewcommand\cite{\citep}  % to get "(Author Year)" with natbib
\newcommand\shortcite{\citeyearpar}% to get "(Year)" with natbib
\newcommand\newcite{\citet} % to get "Author (Year)" with natbib
\newcommand{\citeposs}[1]{\citeauthor{#1}'s (\citeyear{#1})} % to get "Author's (Year)"



\bibliographystyle{acl_natbib}

\usepackage{listings}  
\usepackage{xcolor} 
\usepackage{tcolorbox} 

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}


\lstset{frame=tb,
    language=c++,
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    keepspaces=true,                 
    numbers=left,       
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
}

\usepackage{listings}
\lstdefinestyle{code}{
  basicstyle=\ttfamily\small,
  columns=fullflexible,
  keepspaces=true,
  showstringspaces=false,
  frame=single,
  framerule=0.4pt,
  rulecolor=\color{black!30},
  backgroundcolor=\color{black!3},
  numbers=left,
  numberstyle=\tiny,
  xleftmargin=1em,
  framexleftmargin=1em
}
\newcommand{\code}[1]{\texttt{#1}}




\title{PMPH2025-Flash Attention}

\author{By \\
  Wanjing Hu / Computer Science, KU  \\
  \texttt{fng685@alumni.ku.dk} \\}
% TODO add your name
\begin{document}
\maketitle

\section{Problem Statement}

Flash Attention proposed by Dao-AILab is an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM\cite{dao2022flashattention} \cite{dao2023flashattention}. In this report, we implement Flash Attention(FA) forward pass with different Domain Specific Languages(DSLs): CUDA, CUDA+Cutlass, PyTorch and PyTorch + Triton.

\subsection{What is Attention and what does Flash Attention do}
The calculation process of Attention is as follows:

\[
S = QK^{T} \quad P = \text{Softmax}(S) \quad O = PV
\]

Among them, the Q, K, and V matrices are generated from the input sequence, while S and P are intermediate matrices, and O is the resulting matrix after the Attention computation.

FlashAttention leverages shared memory to reduce accesses to high-bandwidth memory (HBM). It applies tiling to the $Q$, $K$, and $V$ matrices and modifies the softmax computation to operate efficiently on local tile-level results. In FlashAttention v1, the $Q$ matrix is loaded as a whole while $K$ and $V$ are split  during matrix multiplication (split-$K/V$). See Algorithm~\ref{alg:flashattention1}. In FlashAttention v2, $Q$ is instead split across threads or warps (split-$Q$), while $K$ and $V$ remain fixed. See Algorithm~\ref{alg:flashattention2}.

\begin{algorithm}[htbp]
\caption{FlashAttention-1 (Split-K, Parallel over Warps)}
\begin{algorithmic}[1]
\FOR{each block of keys/values $K_j, V_j$}
    \STATE \textbf{in parallel over warps $w$:} \COMMENT{Each warp processes a $Q_i$ tile}
    \STATE \hspace{0.5cm} $S_{\text{local}} = Q_i K_j^{T}$ \COMMENT{partial attention scores}
    \STATE \hspace{0.5cm} $P_{\text{local}} = \text{softmax\_tile}(S_{\text{local}})$ \COMMENT{local normalization (approx.)}
    \STATE \hspace{0.5cm} $O_{\text{partial}} = P_{\text{local}} V_j$ \COMMENT{partial output}
    \STATE \textbf{synchronize and sum across warps} \COMMENT{combine all $O_{\text{partial}}$}
    \STATE \textbf{update running normalization stats} \COMMENT{for true softmax}
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[htbp]
\caption{FlashAttention-2 (Split-Q, Fully Parallel Warps)}
\begin{algorithmic}[1]
\STATE \textbf{in parallel over warps $w$:} \COMMENT{Each warp handles its own $Q_i$ tile}
\STATE \hspace{0.5cm} initialize $O_w = 0$, normalization stats
\FOR{each $K_j, V_j$ block}
    \STATE \hspace{0.5cm} $S_{\text{local}} = Q_w K_j^{T}$
    \STATE \hspace{0.5cm} $P_{\text{local}} = \text{softmax\_tile}(S_{\text{local}})$
    \STATE \hspace{0.5cm} $O_w \mathrel{+}= P_{\text{local}} V_j$ \COMMENT{accumulate output locally}
\ENDFOR
\STATE \hspace{0.5cm} finalize normalization
\STATE \hspace{0.5cm} write $O_w$ to global output
\end{algorithmic}
\end{algorithm}

\subsection{Work-Depth Complexity}

Here we count only matrix multiplication for FLOPs calculation, since $Softmax$ involves much fewer FLOPs. For a matrix $A$ of size $m \times n$ and a matrix $B$ of size $n \times p$,  the multiplication $A \times B$ requires $m \times n \times p$ multiplications and $m \times n \times p$ additions, totaling $2mnp$ FLOPs.

Let $N$ be the sequence length, $b$ be the batch size, $h$ be the head number and $d$ be the head dimension.

\textbf{Work Complexity: } The work complexity is unified with Attention\footnote{Note that we ignore the $Q=xW_Q$, $K=xW_K$, $V=xW_V$, $O=xW_O$ since we did not implement that in code. This is aligned with in original flash attention 2 paper on page 10.}.


\textbf{Score ($S = QK^T$):} This computes the $N \times N$ score matrix.

Size: $(b, h, N, d) \times (b, h, d, N) \rightarrow (b, h, N, N)$

FLOPS: $2 \cdot b \cdot h \cdot N \cdot d \cdot N = 2 \cdot b \cdot (h \cdot d) \cdot N^2$

\textbf{Output ($O = PV$):} This computes the $N \times d$ output matrix.

Size: $(b, h, N, N) \times (b, h, N, d) \rightarrow (b, h, N, d)$

FLOPS: $2 \cdot b \cdot h \cdot N \cdot N \cdot d = 2 \cdot b \cdot (h \cdot d) \cdot N^2$

$$ Total\, FLOPS = (2 \cdot b \cdot d \cdot N^2) + (2 \cdot b \cdot d \cdot N^2) = 4 \cdot b \cdot d \cdot N^2 $$


\textbf{Depth Complexity: } Different tiling strategy brings different depth complexity. See each one's session for more details.


\subsection{Evaluation Metrics}
We focus on latency speedup, compute efficiency, and memory efficiency.\footnote{We intended to measure the achieved memory bandwidth, but were unable to install \textit{Nsight Compute} successfully.} Latency speedup is assessed using kernel runtime of the execution time $(T)$ of the self-attention forward. Here our baseline is a naive Attention implementation with no shared memory or warp-level optimizations ($\text{Speedup} = \frac{T_{\text{Base}}}{T_{\text{FlashAttention}}}$).

For compute efficiency we use achieved Tera Floating-Point Operations Per Second(TFLOPs/s): 

$$\text{Achieved TFLOPs/s} = \frac{\text{Total Floating-Point operations in FA}}{T_{\text{Execution Time}}}$$.

We compare the achieved TFLOPs/s with the state-of-art TFLOPS in FA papers to see if we are using the GPU compute resource efficiently. The achieved TFLOPs/s of FA1 in paper is 25-40\% of the theoretical matmul TFLOPs/s with FP16/BF16 on A100. And for FA2 is 50\%-73\% of that\cite{dao2023flashattention}. Specifically, when head dimension is 128, FA2 can achieve 225 TFLOPs/s in forward pass.


%For memory efficiency we access with achieved bandwidth. We use Nsight Compute to see the total runtime bandwidth.

%$$\text{Achieved Bandwidth} = \frac{\text{Total runtime memory access in FA}}{T_{\text{Execution Time}}}$$.

 \begin{comment}
\begin{algorithm}[htbp]
  \caption{Standard Attention}
  \label{alg:standard_attention}
  \begin{algorithmic}[1]
    \STATE Load $Q$ and $K$ by blocks from HBM.
    \STATE Compute $S = (1/\sqrt{d})QK^T$ (GEMM-I).
    \STATE Write $S$ to HBM.
    \STATE Read $S$ from HBM.
    \STATE Compute $S = S - \text{rowmax}(S)$.
    \STATE Compute $P = \text{softmax}(S)$.
    \STATE Write $P$ to HBM.
    \STATE Load $P$ and $V$ by blocks from HBM.
    \STATE Compute $O = PV$ (GEMM-II).
    \STATE Write $O$ to HBM.
  \end{algorithmic}
\end{algorithm}
\end{comment}

%%%%%%% TODO Implementation part %%%%%%%%%%%
% Answer the following 3 questions in each one's own session
% 1) validation: how did you implement the validation (rationale), on what datasets have you tried, what are the corner cases, etc.
% 2) memory access: what are the shared memory and register resources that are used per block. How many accesses to global memory are performed.
% 3) evaluation: performance should be evaluated for various datasets; is the performance sensitive to dataset characteristics?  (at least to size it should). Evaluate the impact of various parameters, such as Q (the number of elements processed sequentially per thread), B the block size, etc. Evaluate the impact of optimizations and compare with state of the art approaches, and if possible with ideal performace (ideal peak bandwidth performance). 

\section{CUDA by Zhigao Yan}

\section{CUDA + Cutlass by fng685}


\section{CUTLASS-based FlashAttention Implementation}

\subsection{Implementation and Validation}
This implementation utilizes CUTLASS WMMA (Warp-Matrix-Multiply-Accumulate) for the $S=QK^{T}$ operation. The softmax calculation is implemented using warp-level shuffle operations. WMMA inputs are \texttt{fp16}, while an \texttt{fp32} accumulator is used to maintain precision.

\textbf{Correctness:} The kernel's output was validated against a naive CUDA implementation using identical inputs, demonstrating a relative error of less than 2\%. This minor deviation is expected due to the use of \texttt{fp16} for data representation and intermediate calculations, which introduces accumulation errors.

\textbf{Dataset:} Test cases were generated using randomly shaped tensors (weights and inputs) populated with random \texttt{fp16} values to prevent overflow. The shapes varied in sequence length ($N$), head number ($H$), and head dimension ($D$) to benchmark performance and approach the maximum TFLOPs/s. Based on the evaluation tables, all test cases use a fixed batch size of 1.

\textbf{Parameter Choice:} An additional key parameter in the FlashAttention algorithm is the tile size. In addition to the naive baseline, a non-CUTLASS FlashAttention was first implemented with two different tile sizes ($45 \times 90$ and $120 \times 120$) to identify a suitable configuration. These tile sizes were selected to fit within the 192KB shared memory limit per Streaming Multiprocessor (SM) of the A100-40G GPU \cite{nvidia2022a100}. The memory footprint for a given tile is calculated as shown in Section~\ref{sec:mem-acc}.

\subsection{Depth Complexity}
% This section has been rewritten, as the original analysis incorrectly equated
% work complexity (O(N^2)) with parallel depth complexity.

The parallel depth of an algorithm represents the longest chain of dependent operations. Let $T_r = N / B_r$ and $T_c = N / B_c$ be the number of query and key/value blocks, respectively, where $B_r$ and $B_c$ are the tile dimensions.

The FlashAttention algorithm parallelizes the outer loop over $T_r$ (the query blocks) by assigning each block to a separate CUDA thread block. These blocks execute in parallel.

Within each thread block (processing one $Q$ tile), the algorithm must iterate \textit{sequentially} through all $T_c$ blocks of $K$ and $V$. The computation within this inner loop (a tile-matrix multiply and the warp-reduce softmax) has a near-constant depth $D_{\text{inner}}$, or $O(\log B_c)$ for the reduction.

The total parallel depth is therefore dominated by the sequential inner loop, as all outer loop blocks run in parallel.
\[
D_{\text{FA1}} = O(T_c \times D_{\text{inner}}) \approx O(T_c) = O(N / B_c) \approx O(N).
\]
This $O(N)$ depth is a fundamental improvement over the $O(N^2)$ \textit{work} complexity, which the naive implementation also suffers from in terms of depth due to its large matrix materialization.

\subsection{Shared Memory Access}
\label{sec:mem-acc}

Shared memory is used to store the tiles of $Q$, $K$, and $V$, as well as the intermediate results for $S$, $P$, and the output accumulator $O$. It also stores the online softmax statistics (the row-wise maximum $m$ and sum of exponentials $l$).
\begin{lstlisting}
static constexpr size_t get_smem_size() {
    return (kTileM * kHeadDim + kTileN * kHeadDim * 2) * sizeof(cutlass::half_t) +
           (kTileM * kTileN * 2) * sizeof(float) +  // S and P
           (kTileM * 2) * sizeof(float) +          // m, row-wise max; l, row-wise sum of exponentials
           (kTileM * kHeadDim) * sizeof(float);   // O_accum
}
\end{lstlisting}

\subsection{Evaluation and Results}
The evaluation was conducted on an A100-40G GPU, tested on CUDA 12.8 with CUTLASS 3.4.0. The CUDA driver version was $545.23.08$.

The naive implementation relies entirely on HBM and does not utilize shared memory. Consequently, it is memory-bound, with the GPU spending most of its time stalled on HBM accesses rather than performing computation. In contrast, the tiled versions demonstrate significantly lower latency by leveraging shared memory.

The CUTLASS-based implementation (using a $45 \times 90$ tile) achieves the best latency across all test cases. This is attributed to its use of CUTLASS WMMA, which maps operations efficiently to the A100's Tensor Cores using a $16 \times 8 \times 16$ tile layout.

\begin{table}[htbp]
\centering
\caption{Flash Attention Throughput Comparison (TFLOPs/s and Speedup vs. Naive)}
\label{tab:throughput_comparison}
\sisetup{table-number-alignment=center, round-mode=places, round-precision=2}
\begin{tabular}{@{}l S[table-format=1.2] S[table-format=1.2] S[table-format=2.2] S[table-format=1.2] S[table-format=2.2] S[table-format=1.2] S[table-format=2.2]@{}}
\toprule
& & \multicolumn{2}{c}{\textbf{Tile 45x90}} & \multicolumn{2}{c}{\textbf{CUTLASS Tile 45x90}} & \multicolumn{2}{c}{\textbf{Tile 120x120}} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
\textbf{Config (B, H, N, D)} & \multicolumn{1}{c}{\textbf{Naive}} & \multicolumn{1}{c}{\textbf{TFLOPs/s}} & \multicolumn{1}{c}{\textbf{Speedup}} & \multicolumn{1}{c}{\textbf{TFLOPs/s}} & \multicolumn{1}{c}{\textbf{Speedup}} & \multicolumn{1}{c}{\textbf{TFLOPs/s}} & \multicolumn{1}{c}{\textbf{Speedup}} \\
\midrule
(1, 1, 512, 64)    & 0.00  & 0.07  & x & \textbf{0.16} & x & 0.03  & x  \\
(1, 1, 1024, 64)   & 0.01  & 0.14  & 14.00x & \textbf{0.33} & 33.00x & 0.06  & 6.00x  \\
(1, 1, 2048, 64)   & 0.02  & 0.29  & 14.50x & \textbf{0.67} & 33.50x & 0.11  & 5.50x  \\
(1, 1, 4096, 64)   & 0.03  & 0.58  & 19.33x & \textbf{1.06} & 35.33x & 0.23  & 7.67x  \\
(1, 1, 8192, 64)   & 0.07  & 0.91  & 13.00x & \textbf{2.13} & 30.43x & 0.46  & 6.57x  \\
(1, 32, 8192, 32)  & 0.31   & 1.04   & 3.35x   & \textbf{2.23}  & 7.19x   & 0.38   & 1.23x   \\
(1, 32, 8192, 64)  & 0.26  & 0.92  & 3.54x  & \textbf{3.08} & 11.85x  & 0.62  & 2.38x  \\
(1, 32, 8192, 128) & 0.27 & 1.07 & 3.96x & \textbf{3.02} & 11.19x & 0.77 & 2.85x \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[htbp]
\centering
\caption{FA1xCutlass Latency (Runtime in ms and Speedup vs. Naive)}
\label{tab:full_comparison}
\sisetup{table-number-alignment=center, round-mode=places, round-precision=2}
\begin{tabular}{@{}l S[table-format=4.2] S[table-format=4.2] S[table-format=2.2] S[table-format=4.2] S[table-format=2.2] S[table-format=4.2] S[table-format=2.2]@{}}
\toprule
& & \multicolumn{2}{c}{\textbf{Tile 45x90}} & \multicolumn{2}{c}{\textbf{CUTLASS Tile 45x90}} & \multicolumn{2}{c}{\textbf{Tile 120x120}} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
\textbf{Config (B, H, N, D)} & \mcc{\textbf{Naive}} & \mcc{\textbf{Time (ms)}} & \mcc{\textbf{Speedup}} & \mcc{\textbf{Time (ms)}} & \mcc{\textbf{Speedup}} & \mcc{\textbf{Time (ms)}} & \mcc{\textbf{Speedup}} \\
\midrule
(1, 1, 512, 64)    & 13.87  & 0.95  & 14.60x & \textbf{0.42} & 33.02x & 2.35  & 5.90x \\
(1, 1, 1024, 64)   & 27.38  & 1.90  & 14.41x & \textbf{0.81} & 33.80x & 4.69  & 5.84x \\
(1, 1, 2048, 64)   & 53.80  & 3.72  & 14.46x & \textbf{1.60} & 33.63x & 9.35  & 5.75x \\
(1, 1, 4096, 64)   & 128.08 & 7.36  & 17.40x & \textbf{4.05} & 31.62x & 18.68 & 6.85x \\
(1, 1, 8192, 64)   & 243.31 & 18.79  & 12.95x & \textbf{8.05} & 30.22x & 37.15 & 6.55x \\
(1, 32, 8192, 32)  & 899.36 & 264.30 & 3.40x  & \textbf{123.37} & 7.29x  & 722.78 & 1.24x \\
(1, 32, 8192, 64)  & 2151.82 & 597.63 & 3.60x  & \textbf{178.72} & 12.04x  & 891.22 & 2.41x \\
(1, 32, 8192, 128) & 4141.44 & 1023.15 & 4.05x  & \textbf{364.54} & 11.36x  & 1421.33 & 2.91x \\
\bottomrule
\end{tabular}
\end{table}

\section{PyTorch by Yuxuan Chen}

\section{PyTorch + Triton by Hongyang Cui}


\bibliography{custom}

\end{document}
