\pdfoutput=1

\documentclass[11pt]{article}


\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{graphicx}

\usepackage{natbib}


\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{etoolbox}
\usepackage[table]{xcolor}
\usepackage[margin=1in]{geometry}

% Define commands for highlighting the minimum value in a row
\newcolumntype{R}{S[table-format=4.3]}
\newrobustcmd{\B}[1]{%
  \ifdimless{#1pt}{9999pt}{\bfseries\color{blue!75!black}}{}%
  #1
}
\newcommand{\mcc}[1]{\multicolumn{1}{c}{#1}}

% for citation commands in the .tex, authors can use:
% \citep, \citet, and \citeyearpar for compatibility with natbib, or
% \cite, \newcite, and \shortcite for compatibility with older ACL .sty files
\renewcommand\cite{\citep}  % to get "(Author Year)" with natbib
\newcommand\shortcite{\citeyearpar}% to get "(Year)" with natbib
\newcommand\newcite{\citet} % to get "Author (Year)" with natbib
\newcommand{\citeposs}[1]{\citeauthor{#1}'s (\citeyear{#1})} % to get "Author's (Year)"



\bibliographystyle{acl_natbib}


\usepackage{listings}  
\usepackage{xcolor} 
\usepackage{tcolorbox} 

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}


\lstset{frame=tb,
    language=c++,
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    keepspaces=true,                 
    numbers=left,       
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
}

\usepackage{listings}
\lstdefinestyle{code}{
  basicstyle=\ttfamily\small,
  columns=fullflexible,
  keepspaces=true,
  showstringspaces=false,
  frame=single,
  framerule=0.4pt,
  rulecolor=\color{black!30},
  backgroundcolor=\color{black!3},
  numbers=left,
  numberstyle=\tiny,
  xleftmargin=1em,
  framexleftmargin=1em
}
\newcommand{\code}[1]{\texttt{#1}}




\title{PMPH2025-Flash Attention}

\author{By \\
  Wanjing Hu / Computer Science, KU  \\
  \texttt{fng685@alumni.ku.dk} \\}
% TODO add your name
\begin{document}
\maketitle

\section{Problem Statement}

Flash Attention proposed by Dao-AILab is an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM\cite{dao2022flashattention} \cite{dao2023flashattention}. In this report, we implement Flash Attention(FA) forward pass with different Domain Specific Languages(DSLs): CUDA, CUDA+Cutlass, PyTorch and PyTorch + Triton.

\subsection{What is Attention and what does Flash Attention do}
The calculation process of Attention is as follows:

\[
S = QK^{T} \quad P = \text{Softmax}(S) \quad O = PV
\]

Among them, the Q, K, and V matrices are generated from the input sequence, while S and P are intermediate matrices, and O is the resulting matrix after the Attention computation.

FlashAttention leverages shared memory to reduce accesses to high-bandwidth memory (HBM). It applies tiling to the $Q$, $K$, and $V$ matrices and modifies the softmax computation to operate efficiently on local tile-level results. In FlashAttention v1, the $Q$ matrix is loaded as a whole while $K$ and $V$ are split  during matrix multiplication (split-$K/V$). See Algorithm~\ref{alg:flashattention1}. In FlashAttention v2, $Q$ is instead split across threads or warps (split-$Q$), while $K$ and $V$ remain fixed. See Algorithm~\ref{alg:flashattention2}.

\begin{algorithm}[htbp]
\caption{FlashAttention-1 Block Logic (Warp-Level Split-K)}
\label{alg:flashattention1}
\begin{algorithmic}[1]
    \FOR{each $K_j, V_j$ block}
        \STATE \textbf{in parallel for each warp $w$:}
        \STATE \hspace{0.5cm} $K_{j,w}, V_{j,w} \gets$ slices of $K_j, V_j$
        \STATE \hspace{0.5cm} $S_{ij,w} \gets Q_i K_{j,w}^T$
        \STATE \hspace{0.5cm} $(P_{ij,w}, L_{i,w}, m_{i,w}) \gets \text{update\_stats}(S_{ij,w}, L_{i,w}, m_{i,w})$
        \STATE \hspace{0.5cm} $O_{\text{partial},w} \gets P_{ij,w} V_{j,w}$
        \STATE \hspace{0.5cm} Write $O_{\text{partial},w}$ to $sO_{\text{partial}}$
        \STATE Synchronize all warps
        \STATE $O_{\text{block}} \gets \text{reduce\_sum}(sO_{\text{partial}})$
        \STATE $O_i \gets \text{update\_output}(O_i, O_{\text{block}})$
    \ENDFOR
    \STATE $O_i \gets \text{finalize\_scaling}(O_i, L_i)$
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[htbp]
\caption{FlashAttention-2 Block Logic (Warp-Level Split-Q)}
\label{alg:flashattention2}
\begin{algorithmic}[1]
    \FOR{each $K_j, V_j$ block}
        \STATE \textbf{in parallel for each warp $w$:}
        \STATE \hspace{0.5cm} $Q_{i,w} \gets$ slice of $Q_i$
        \STATE \hspace{0.5cm} $S_{ij,w} \gets Q_{i,w} K_j^T$
        \STATE \hspace{0.5cm} $(P_{ij,w}, L_{i,w}, m_{i,w}) \gets \text{update\_stats}(S_{ij,w}, L_{i,w}, m_{i,w})$
        \STATE \hspace{0.5cm} $O_{i,w} \gets \text{update\_output}(O_{i,w}, P_{ij,w} V_j)$
    \ENDFOR
    \STATE $O_i \gets \text{finalize\_scaling}(O_i, L_i)$
\end{algorithmic}
\end{algorithm}

\subsection{Work-Depth Complexity}

Here we count only matrix multiplication for FLOPs calculation, since $Softmax$ involves much fewer FLOPs. For a matrix $A$ of size $m \times n$ and a matrix $B$ of size $n \times p$,  the multiplication $A \times B$ requires $m \times n \times p$ multiplications and $m \times n \times p$ additions, totaling $2mnp$ FLOPs.

Let $N$ be the sequence length, $b$ be the batch size, $h$ be the head number and $d$ be the head dimension.

\textbf{Work Complexity: } The work complexity is unified with Attention\footnote{Note that we ignore the $Q=xW_Q$, $K=xW_K$, $V=xW_V$, $O=xW_O$ since we did not implement that in code. This is aligned with in original flash attention 2 paper on page 10.}.


\textbf{Score ($S = QK^T$):} This computes the $N \times N$ score matrix.

Size: $(b, h, N, d) \times (b, h, d, N) \rightarrow (b, h, N, N)$

FLOPS: $2 \cdot b \cdot h \cdot N \cdot d \cdot N = 2 \cdot b \cdot (h \cdot d) \cdot N^2$

\textbf{Output ($O = PV$):} This computes the $N \times d$ output matrix.

Size: $(b, h, N, N) \times (b, h, N, d) \rightarrow (b, h, N, d)$

FLOPS: $2 \cdot b \cdot h \cdot N \cdot N \cdot d = 2 \cdot b \cdot (h \cdot d) \cdot N^2$

$$ Total\, FLOPS = (2 \cdot b \cdot d \cdot N^2) + (2 \cdot b \cdot d \cdot N^2) = 4 \cdot b \cdot d \cdot N^2 $$


\textbf{Depth Complexity: } Different tiling strategy brings different depth complexity. See each one's session for more details.


\subsection{Evaluation Metrics}
We focus on latency speedup, compute efficiency, and memory efficiency.\footnote{We intended to measure the achieved memory bandwidth, but were unable to install \textit{Nsight Compute} successfully.} Latency speedup is assessed using kernel runtime of the execution time $(T)$ of the self-attention forward. Here our baseline is a naive Attention implementation with no shared memory or warp-level optimizations ($\text{Speedup} = \frac{T_{\text{Base}}}{T_{\text{FlashAttention}}}$).

For compute efficiency we use achieved Tera Floating-Point Operations Per Second(TFLOPs/s): 

$$\text{Achieved TFLOPs/s} = \frac{\text{Total Floating-Point operations in FA}}{T_{\text{Execution Time}}}$$.

We compare the achieved TFLOPs/s with the state-of-art TFLOPS in FA papers to see if we are using the GPU compute resource efficiently. The achieved TFLOPs/s of FA1 in paper is 25-40\% of the theoretical matmul TFLOPs/s with FP16/BF16 on A100. And for FA2 is 50\%-73\% of that\cite{dao2023flashattention}. Specifically, when head dimension is 128, FA2 can achieve 227 TFLOPs/s in forward pass.


%For memory efficiency we access with achieved bandwidth. We use Nsight Compute to see the total runtime bandwidth.

%$$\text{Achieved Bandwidth} = \frac{\text{Total runtime memory access in FA}}{T_{\text{Execution Time}}}$$.

 \begin{comment}
\begin{algorithm}[htbp]
  \caption{Standard Attention}
  \label{alg:standard_attention}
  \begin{algorithmic}[1]
    \STATE Load $Q$ and $K$ by blocks from HBM.
    \STATE Compute $S = (1/\sqrt{d})QK^T$ (GEMM-I).
    \STATE Write $S$ to HBM.
    \STATE Read $S$ from HBM.
    \STATE Compute $S = S - \text{rowmax}(S)$.
    \STATE Compute $P = \text{softmax}(S)$.
    \STATE Write $P$ to HBM.
    \STATE Load $P$ and $V$ by blocks from HBM.
    \STATE Compute $O = PV$ (GEMM-II).
    \STATE Write $O$ to HBM.
  \end{algorithmic}
\end{algorithm}
\end{comment}

%%%%%%% TODO Implementation part %%%%%%%%%%%
% Answer the following 3 questions in each one's own session
% 1) validation: how did you implement the validation (rationale), on what datasets have you tried, what are the corner cases, etc.
% 2) memory access: what are the shared memory and register resources that are used per block. How many accesses to global memory are performed.
% 3) evaluation: performance should be evaluated for various datasets; is the performance sensitive to dataset characteristics?  (at least to size it should). Evaluate the impact of various parameters, such as Q (the number of elements processed sequentially per thread), B the block size, etc. Evaluate the impact of optimizations and compare with state of the art approaches, and if possible with ideal performace (ideal peak bandwidth performance). 

\section{CUDA by Zhigao Yan}

\section{CUDA + Cutlass by Wanjing Hu}

\subsection{Validation of implementation}

\textbf{Correctness} The output of the new kernel is compared with CUDA naive implementation with the same data input, with precision of diff less than 2\%. Since we are using float point 16 as the data calculate precision, there will be some accumulation error.


\subsection{Depth Complexity}

Let $T_r$ and $T_c$ be the number of blocks along the sequence length $N$ (i.e., $T_r \approx O(N)$ and $T_c \approx O(N)$).


For FA1, since one block must sequentially iterate over both the $T_r$ row-blocks and $T_c$ column-blocks, the critical path (depth) is proportional to the total number of tiles it must compute:
              \[ D_{FA1} = O(T_r \times T_c) = O(N^2) \]

For FA2, it improves parallelism by \textit{adding} the sequence length dimension (Q-blocks) to the grid-level parallelization. The $T_r$ row-blocks are processed by different thread blocks simultaneously. Thus the critical path is now only the sequential inner loop (which is the online softmax update) that each thread block must perform. This loop iterates over the $T_c$ column-blocks:
              \[ D_{FA2} = O(T_c) = O(N) \]



\subsection{Memory Access}

I use \textbf{shared memory} for the tile of $Q$, $K$ and $V$, and the intermediate result of $S$, $P$, and $O$. Also for storing the max and sum of each row in $S$, where $S=QK^T$.
\begin{lstlisting}
// lines 93-98
static constexpr size_t get_smem_size() {
    return (kTileM * kHeadDim + kTileN * kHeadDim * 2) * sizeof(cutlass::half_t) +
           (kTileM * kTileN * 2) * sizeof(float) +  // S and P
           (kTileM * 2) * sizeof(float) +          // m, max of the row; l, for sum of the row of S_ij
           (kTileM * kHeadDim) * sizeof(float);   // O_accum
}
\end{lstlisting}

\begin{table}[h!]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Data Structure} & \textbf{Dimensions} & \textbf{Elements} & \textbf{Bytes} & \textbf{Calculation} \\
\midrule
\textbf{Q} & [45, 32] & 1,440 & 2,880 B & $45 \times 32 \times 2$ \\
\textbf{K} & [90, 32] & 2,880 & 5,760 B & $90 \times 32 \times 2$ \\
\textbf{V} & [90, 32] & 2,880 & 5,760 B & $90 \times 32 \times 2$ \\
\textbf{S} & [45, 90] & 4,050 & 16,200 B & $45 \times 90 \times 4$ \\
\textbf{P} & [45, 90] & 4,050 & 16,200 B & $45 \times 90 \times 4$ \\
\textbf{m} & [45] & 45 & 180 B & $45 \times 4$ \\
\textbf{l} & [45] & 45 & 180 B & $45 \times 4$ \\
\textbf{O\_accum} & [45, 32] & 1,440 & 5,760 B & $45 \times 32 \times 4$ \\
\midrule
\textbf{TOTAL} & - & - & \textbf{52,920 B} & $\approx$ \textbf{51.7 KB} \\
\bottomrule
\end{tabular}
\caption{SMEM breakdown for HEAD\_DIM=32.}
\end{table}

\subsection{Memory Access Analysis}


\subsection{Evaluation}
According to the work complexity analysis, the profiling is based on different sequence length and 

\begin{table}[htbp]
\centering
\caption{Flash Attention Throughput Comparison (TFLOPs/s and Speedup vs. Naive)}
\label{tab:throughput_comparison}
\sisetup{table-number-alignment=center, round-mode=places, round-precision=2}
\begin{tabular}{@{}l S[table-format=1.2] S[table-format=1.2] S[table-format=2.2] S[table-format=1.2] S[table-format=2.2] S[table-format=1.2] S[table-format=2.2]@{}}
\toprule
& & \multicolumn{2}{c}{\textbf{Tile 45x90}} & \multicolumn{2}{c}{\textbf{CUTLASS Tile 45x90}} & \multicolumn{2}{c}{\textbf{Large Tile}} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
\textbf{Config (B, H, N, D)} & \multicolumn{1}{c}{\textbf{Naived}} & \multicolumn{1}{c}{\textbf{TFLOPs/s}} & \multicolumn{1}{c}{\textbf{Speedup}} & \multicolumn{1}{c}{\textbf{TFLOPs/s}} & \multicolumn{1}{c}{\textbf{Speedup}} & \multicolumn{1}{c}{\textbf{TFLOPs/s}} & \multicolumn{1}{c}{\textbf{Speedup}} \\
\midrule
(1, 1, 512, 64)    & 0.00  & 0.07  & x & \textbf{0.10} & 19.63x & 0.03  & x  \\
(1, 1, 1024, 64)   & 0.01  & 0.14  & 14x & \textbf{0.19} & 19.76x & 0.06  & 6.00x  \\
(1, 1, 2048, 64)   & 0.02  & 0.29  & 14.5x & \textbf{0.39} & 19.63x & 0.11  & 5.50x  \\
(1, 1, 4096, 64)   & 0.03  & 0.58  & 19.33x & \textbf{0.68} & 20.43x & 0.23  & 7.67x  \\
(1, 32, 8192, 32)  & 0.31   & 1.04   & 3.35x   & \textbf{1.12}  & 3.66x   & 0.38   & 1.23x   \\
(1, 32, 8192, 64)  & 0.26  & 0.92  & 3.54x  & \textbf{1.77} & 6.94x  & 0.62  & 2.38x  \\
(1, 32, 8192, 128) & 0.27 & 1.07 & 3.96x & \textbf{1.81} & 6.83x & 0.77 & 2.85x \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[htbp]
\centering
\caption{FA1xCutlass Latency (Runtime in ms and compare vs. Naive)}
\label{tab:full_comparison}
\sisetup{table-number-alignment=center, round-mode=places, round-precision=2}
\begin{tabular}{@{}l S[table-format=4.2] S[table-format=4.2] S[table-format=2.2] S[table-format=4.2] S[table-format=2.2] S[table-format=4.2] S[table-format=2.2]@{}}
\toprule
& & \multicolumn{2}{c}{\textbf{Tile 45x90}} & \multicolumn{2}{c}{\textbf{CUTLASS Tile 45x90}} & \multicolumn{2}{c}{\textbf{Tile 120x120}} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
\textbf{Config (B, H, N, D)} & \mcc{\textbf{Naive}} & \mcc{\textbf{Time (ms)}} & \mcc{\textbf{Speedup}} & \mcc{\textbf{Time (ms)}} & \mcc{\textbf{Speedup}} & \mcc{\textbf{Time (ms)}} & \mcc{\textbf{Speedup}} \\
\midrule
(1, 1, 512, 64)    & 13.87  & 0.95  & 14.62x & \B{0.71} & 19.63x & 2.35  & 5.89x \\
(1, 1, 1024, 64)   & 27.38  & 1.90  & 14.42x & \B{1.39} & 19.76x & 4.69  & 5.84x \\
(1, 1, 2048, 64)   & 53.80  & 3.72  & 14.47x & \B{2.74} & 19.63x & 9.35  & 5.75x \\
(1, 1, 4096, 64)   & 128.08 & 7.36  & 17.40x & \B{6.27} & 20.43x & 18.68 & 6.86x \\
(1, 32, 8192, 32)  & 899.36 & 264.30 & 3.40x  & \B{245.50} & 3.66x  & 722.78 & 1.24x \\
(1, 32, 8192, 64)  & 2151.82 & 597.63 & 3.60x  & \B{309.99} & 6.94x  & 891.22 & 2.41x \\
(1, 32, 8192, 128) & 4141.44 & 1023.15 & 4.05x  & \B{606.79} & 6.83x  & 1421.33 & 2.91x \\
\bottomrule
\end{tabular}
\end{table}

\section{PyTorch by Yuxuan Chen}

\section{PyTorch + Triton by Hongyang Cui}


\bibliography{custom}

\end{document}
