\pdfoutput=1

\documentclass[11pt]{article}


\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{graphicx}

\usepackage{natbib}
% for citation commands in the .tex, authors can use:
% \citep, \citet, and \citeyearpar for compatibility with natbib, or
% \cite, \newcite, and \shortcite for compatibility with older ACL .sty files
\renewcommand\cite{\citep}  % to get "(Author Year)" with natbib
\newcommand\shortcite{\citeyearpar}% to get "(Year)" with natbib
\newcommand\newcite{\citet} % to get "Author (Year)" with natbib
\newcommand{\citeposs}[1]{\citeauthor{#1}'s (\citeyear{#1})} % to get "Author's (Year)"

\bibliographystyle{acl_natbib}


\usepackage{listings}  
\usepackage{xcolor} 
\usepackage{tcolorbox} 

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}


\lstset{frame=tb,
    language=Python,
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    keepspaces=true,                 
    numbers=left,       
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
}

\usepackage{listings}
\lstdefinestyle{code}{
  basicstyle=\ttfamily\small,
  columns=fullflexible,
  keepspaces=true,
  showstringspaces=false,
  frame=single,
  framerule=0.4pt,
  rulecolor=\color{black!30},
  backgroundcolor=\color{black!3},
  numbers=left,
  numberstyle=\tiny,
  xleftmargin=1em,
  framexleftmargin=1em
}
\newcommand{\code}[1]{\texttt{#1}}




\title{PMPH2025-Flash Attention}

\author{By \\
  Wanjing Hu / Computer Science, KU  \\
  \texttt{fng685@alumni.ku.dk} \\}

\begin{document}
\maketitle

\section{Problem Statement}

Flash Attention proposed by Dao-AILab is an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM\cite{dao2022flashattention}. In this report, we implement Flash Attention 1(FA1) forward calculation with different Domain Specific Languages(DSLs).

For \textbf{evaluation metrics}, we focus on Latency speed up across different DSLs, compute efficiency, and memory efficiency.

Latency speedup is assessed using kernel runtime of the execution time $(T)$ of the self-attention forward. Here we choose Baseline as TODO: 

$$\text{Speedup} = \frac{T_{\text{Base}}}{T_{\text{FlashAttention}}}.$$

For compute efficiency we use achieved Tera Floating-Point Operations Per Second(TFLOPs//s): 

$$\text{Achieved TFLOPs} = \frac{\text{Total Floating-Point operations in FA1}}{T_{\text{Execution Time}}}$$.

We compare the achieved TFLOPs with the theoretical hardware TFLOPs to see if we are using the GPU compute resource efficiently.

For memory efficiency we access with achieved bandwidth. We use Nsight Compute to see the total runtime bandwidth.

$$\text{Achieved Bandwidth} = \frac{\text{Total runtime memory access in FA}}{T_{\text{Execution Time}}}$$.

\section{DSL XX by Your Name TODO}
\section{DSL XX by Your Name TODO}
\section{DSL XX by Your Name TODO}
\section{DSL XX by Your Name TODO}


\bibliography{custom}


\end{document}
